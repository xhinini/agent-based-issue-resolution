=== Original Patch ===
diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py
index 8376b487..e5697ad0 100644
--- a/xarray/core/dataset.py
+++ b/xarray/core/dataset.py
@@ -3907,7 +3907,23 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             dataset.
         """
         merge_result = dataset_update_method(self, other)
-        return self._replace(inplace=True, **merge_result._asdict())
+        updated_dataset = self._replace(inplace=True, **merge_result._asdict())
+        
+        # Ensure chunking is preserved for dask arrays
+        new_variables = {}
+        for var_name, var in updated_dataset.variables.items():
+            original_var = self.variables.get(var_name)
+            if original_var is not None and hasattr(original_var.data, 'chunks') and hasattr(var.data, 'chunks'):
+                # Re-chunk the updated variable to match the original chunks
+                new_chunks = {
+                    dim: original_var.chunks[original_var.get_axis_num(dim)]
+                    for dim in var.dims
+                    if dim in original_var.dims
+                }
+                new_variables[var_name] = var.chunk(new_chunks)
+            else:
+                new_variables[var_name] = var
+        return updated_dataset._replace(variables=new_variables)
 
     def merge(
         self,

=== Fixed Patch ===
diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py
index 8376b487..e5697ad0 100644
--- a/xarray/core/dataset.py
+++ b/xarray/core/dataset.py
@@ -3907,7 +3907,23 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
             dataset.
         """
         merge_result = dataset_update_method(self, other)
-        return self._replace(inplace=True, **merge_result._asdict())
+updated_dataset = self._replace(inplace=True, **merge_result._asdict())
+        
+        # Ensure chunking is preserved for dask arrays
+new_variables = {}
+        for var_name, var in updated_dataset.variables.items():
+original_var = self.variables.get(var_name)
+            if original_var is not None and hasattr(original_var.data, 'chunks') and hasattr(var.data, 'chunks'):
+                # Re-chunk the updated variable to match the original chunks
+                new_chunks = {
+dim: original_var.chunks[original_var.get_axis_num(dim)]
+                    for dim in var.dims
+                    if dim in original_var.dims
+                }
+new_variables[var_name] = var.chunk(new_chunks)
+            else:
+new_variables[var_name] = var
+return updated_dataset._replace(variables=new_variables)
 
     def merge(
         self,

=== Applied Patterns ===

=== Original Error Messages ===
commit: None
LC_ALL: None
libnetcdf: None
netCDF4: None
pydap: None
h5netcdf: None
Nio: None
zarr: None
cftime: None
nc_time_axis: None
PseudoNetCDF: None
rasterio: None
cfgrib: None
iris: None
bottleneck: None
cartopy: None
seaborn: None
numbagg: None
pint: None
conda: None
sphinx: None

=== Test Verification Results ===
Tests Passed: False
