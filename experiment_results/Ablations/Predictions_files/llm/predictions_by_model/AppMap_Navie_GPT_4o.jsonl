{"instance_id": "astropy__astropy-14365", "model_patch": "diff --git a/astropy/io/ascii/qdp.py b/astropy/io/ascii/qdp.py\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@ -60,7 +60,7 @@ def _line_type(line, delimiter=None):\n     ValueError: Unrecognized QDP line...\n     \"\"\"\n     _decimal_re = r\"[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?\"\n-    _command_re = r\"READ [TS]ERR(\\s+[0-9]+)+\"\n+    _command_re = r\"(?i)READ [TS]ERR(\\s+[0-9]+)+\"  # Add (?i) for case insensitivity\n \n     sep = delimiter\n     if delimiter is None:\n@@ -69,11 +69,10 @@ def _line_type(line, delimiter=None):\n     _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n     _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n     _line_type_re = re.compile(_type_re)\n-    line = line.strip()\n+    line = line.strip().upper()  # Convert the line to uppercase\n     if not line:\n         return \"comment\"\n     match = _line_type_re.match(line)\n-\n     if match is None:\n         raise ValueError(f\"Unrecognized QDP line: {line}\")\n     for type_, val in match.groupdict().items():", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "astropy__astropy-6938", "model_patch": "diff --git a/astropy/io/fits/fitsrec.py b/astropy/io/fits/fitsrec.py\n--- a/astropy/io/fits/fitsrec.py\n+++ b/astropy/io/fits/fitsrec.py\n@@ -526,9 +526,8 @@ class FITS_rec(np.recarray):\n             return\n \n         if isinstance(key, slice):\n-            end = min(len(self), key.stop or len(self))\n-            end = max(0, end)\n-            start = max(0, key.start or 0)\n+            end = min(len(self), key.stop if key.stop is not None else len(self))\n+            start = max(0, key.start if key.start is not None else 0)\n             end = min(end, start + len(value))\n \n             for idx in range(start, end):\n@@ -544,7 +543,7 @@ class FITS_rec(np.recarray):\n                     self.field(idx)[key] = value[idx]\n             else:\n                 raise ValueError('Input tuple or list required to have {} '\n-                                 'elements.'.format(self._nfields))\n+                                 'elements.'.format(self._nfields)) from None\n         else:\n             raise TypeError('Assignment requires a FITS_record, tuple, or '\n                             'list as input.')\n@@ -1261,7 +1260,7 @@ class FITS_rec(np.recarray):\n \n         # Replace exponent separator in floating point numbers\n         if 'D' in format:\n-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n+            output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))\n \n \n def _get_recarray_field(array, key):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "astropy__astropy-7746", "model_patch": "diff --git a/astropy/wcs/src/astropy_wcs.c b/astropy/wcs/src/astropy_wcs.c\n--- a/astropy/wcs/src/astropy_wcs.c\n+++ b/astropy/wcs/src/astropy_wcs.c\n@@ -235,6 +235,80 @@ Wcs_all_pix2world(\n     goto exit;\n   }\n \n+  // Handle empty input\n+  if (PyArray_SIZE(pixcrd) == 0) {\n+    world = (PyArrayObject*)PyArray_SimpleNew(2, PyArray_DIMS(pixcrd), NPY_DOUBLE);\n+    if (world == NULL) {\n+        goto exit;\n+    }\n+    Py_XDECREF(pixcrd);\n+    return (PyObject*)world;\n+  }\n+\n+  world = (PyArrayObject*)PyArray_SimpleNew(2, PyArray_DIMS(pixcrd), NPY_DOUBLE);\n+  if (world == NULL) {\n+    goto exit;\n+  }\n+\n+  /* Make the call */\n+  Py_BEGIN_ALLOW_THREADS\n+  preoffset_array(pixcrd, origin);\n+  wcsprm_python2c(self->x.wcs);\n+  status = pipeline_all_pixel2world(&self->x,\n+                                    (unsigned int)PyArray_DIM(pixcrd, 0),\n+                                    (unsigned int)PyArray_DIM(pixcrd, 1),\n+                                    (double*)PyArray_DATA(pixcrd),\n+                                    (double*)PyArray_DATA(world));\n+  wcsprm_c2python(self->x.wcs);\n+  unoffset_array(pixcrd, origin);\n+  Py_END_ALLOW_THREADS\n+  /* unoffset_array(world, origin); */\n+\n+ exit:\n+  Py_XDECREF(pixcrd);\n+\n+  if (status == 0 || status == 8) {\n+    return (PyObject*)world;\n+  } else {\n+    Py_XDECREF(world);\n+    if (status == -1) {\n+      PyErr_SetString(\n+        PyExc_ValueError,\n+        \"Wrong number of dimensions in input array.  Expected 2.\");\n+      return NULL;\n+    } else {\n+      if (status == -1) {\n+        /* exception already set */\n+        return NULL;\n+      } else {\n+        wcserr_to_python_exc(self->x.err);\n+        return NULL;\n+      }\n+    }\n+  }\n+}\n+\n+  if (!PyArg_ParseTupleAndKeywords(\n+          args, kwds, \"Oi:all_pix2world\", (char **)keywords,\n+          &pixcrd_obj, &origin)) {\n+    return NULL;\n+  }\n+\n+  naxis = self->x.wcs->naxis;\n+\n+  pixcrd = (PyArrayObject*)PyArray_ContiguousFromAny(pixcrd_obj, NPY_DOUBLE, 2, 2);\n+  if (pixcrd == NULL) {\n+    return NULL;\n+  }\n+\n+  if (PyArray_DIM(pixcrd, 1) < naxis) {\n+    PyErr_Format(\n+      PyExc_RuntimeError,\n+      \"Input array must be 2-dimensional, where the second dimension >= %d\",\n+      naxis);\n+    goto exit;\n+  }\n+\n   world = (PyArrayObject*)PyArray_SimpleNew(2, PyArray_DIMS(pixcrd), NPY_DOUBLE);\n   if (world == NULL) {\n     goto exit;\ndiff --git a/astropy/wcs/wcs.py b/astropy/wcs/wcs.py\n--- a/astropy/wcs/wcs.py\n+++ b/astropy/wcs/wcs.py\n@@ -1349,13 +1349,18 @@ reduce these to 2 dimensions using the naxis kwarg.\n     def wcs_pix2world(self, *args, **kwargs):\n         if self.wcs is None:\n             raise ValueError(\"No basic WCS settings were created.\")\n+    \n+        # Check for empty arrays and return empty results if so\n+        if any(isinstance(arg, np.ndarray) and arg.size == 0 for arg in args):\n+            return tuple(np.empty_like(arg) for arg in args if isinstance(arg, np.ndarray))\n+    \n         return self._array_converter(\n             lambda xy, o: self.wcs.p2s(xy, o)['world'],\n             'output', *args, **kwargs)\n     wcs_pix2world.__doc__ = \"\"\"\n         Transforms pixel coordinates to world coordinates by doing\n         only the basic `wcslib`_ transformation.\n-\n+    \n         No `SIP`_ or `distortion paper`_ table lookup correction is\n         applied.  To perform distortion correction, see\n         `~astropy.wcs.WCS.all_pix2world`,\n@@ -2156,6 +2161,11 @@ reduce these to 2 dimensions using the naxis kwarg.\n     def wcs_world2pix(self, *args, **kwargs):\n         if self.wcs is None:\n             raise ValueError(\"No basic WCS settings were created.\")\n+    \n+        # Check for empty arrays and return empty results if so\n+        if any(isinstance(arg, np.ndarray) and arg.size == 0 for arg in args):\n+            return tuple(np.empty_like(arg) for arg in args if isinstance(arg, np.ndarray))\n+    \n         return self._array_converter(\n             lambda xy, o: self.wcs.s2p(xy, o)['pixcrd'],\n             'input', *args, **kwargs)\n@@ -2163,21 +2173,21 @@ reduce these to 2 dimensions using the naxis kwarg.\n         Transforms world coordinates to pixel coordinates, using only\n         the basic `wcslib`_ WCS transformation.  No `SIP`_ or\n         `distortion paper`_ table lookup transformation is applied.\n-\n+    \n         Parameters\n         ----------\n         {0}\n-\n+    \n             For a transformation that is not two-dimensional, the\n             two-argument form must be used.\n-\n+    \n         {1}\n-\n+    \n         Returns\n         -------\n-\n+    \n         {2}\n-\n+    \n         Notes\n         -----\n         The order of the axes for the input world array is determined by\n@@ -2186,30 +2196,30 @@ reduce these to 2 dimensions using the naxis kwarg.\n         `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n         `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n         members can be used to determine the order of the axes.\n-\n+    \n         Raises\n         ------\n         MemoryError\n             Memory allocation failed.\n-\n+    \n         SingularMatrixError\n             Linear transformation matrix is singular.\n-\n+    \n         InconsistentAxisTypesError\n             Inconsistent or unrecognized coordinate axis types.\n-\n+    \n         ValueError\n             Invalid parameter value.\n-\n+    \n         ValueError\n             Invalid coordinate transformation parameters.\n-\n+    \n         ValueError\n             x- and y-coordinate arrays are not the same size.\n-\n+    \n         InvalidTransformError\n             Invalid coordinate transformation parameters.\n-\n+    \n         InvalidTransformError\n             Ill-conditioned coordinate transformation parameters.\n         \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-12589", "model_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -473,13 +473,18 @@ class Query(BaseExpression):\n                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)\n                     del inner_query.annotations[alias]\n                     annotation_select_mask.remove(alias)\n-                # Make sure the annotation_select wont use cached results.\n-                inner_query.set_annotation_mask(inner_query.annotation_select_mask)\n-            if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:\n-                # In case of Model.objects[0:3].count(), there would be no\n-                # field selected in the inner query, yet we must use a subquery.\n-                # So, make sure at least one field is selected.\n-                inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n+                # If alias is reused in select fields, add a distinct one\n+                    if alias in [column.alias for column in outer_query.select]:\n+                        new_alias = alias + '_subq'\n+                        outer_query.annotations[new_alias] = outer_query.annotations.pop(alias)\n+                        alias = new_alias\n+                    # Make sure the annotation_select wont use cached results.\n+                    inner_query.set_annotation_mask(inner_query.annotation_select_mask)\n+                if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:\n+                    # In case of Model.objects[0:3].count(), there would be no\n+                    # field selected in the inner query, yet we must use a subquery.\n+                    # So, make sure at least one field is selected.\n+                    inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n             try:\n                 outer_query.add_subquery(inner_query, using)\n             except EmptyResultSet:", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-12747", "model_patch": "diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -388,6 +388,10 @@ class Collector:\n         # number of objects deleted for each model label\n         deleted_counter = Counter()\n \n+        # Initialize deleted_counter with all involved model labels set to zero\n+        for model in self.data.keys():\n+            deleted_counter[model._meta.label] = 0\n+\n         # Optimize for the case with a single obj and no dependencies\n         if len(self.data) == 1 and len(instances) == 1:\n             instance = list(instances)[0]\n@@ -442,4 +446,10 @@ class Collector:\n         for model, instances in self.data.items():\n             for instance in instances:\n                 setattr(instance, model._meta.pk.attname, None)\n+\n+        # Ensure zero value for any model that did not have deletions\n+        for model in self.data.keys():\n+            if model._meta.label not in deleted_counter:\n+                deleted_counter[model._meta.label] = 0\n+\n         return sum(deleted_counter.values()), dict(deleted_counter)", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-14534", "model_patch": "diff --git a/django/forms/boundfield.py b/django/forms/boundfield.py\n--- a/django/forms/boundfield.py\n+++ b/django/forms/boundfield.py\n@@ -277,7 +277,7 @@ class BoundWidget:\n \n     @property\n     def id_for_label(self):\n-        return 'id_%s_%s' % (self.data['name'], self.data['index'])\n+        return self.data['attrs'].get('id', 'id_%s_%s' % (self.data['name'], self.data['index']))\n \n     @property\n     def choice_label(self):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-14997", "model_patch": "diff --git a/django/db/backends/sqlite3/schema.py b/django/db/backends/sqlite3/schema.py\n--- a/django/db/backends/sqlite3/schema.py\n+++ b/django/db/backends/sqlite3/schema.py\n@@ -238,6 +238,14 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n \n         constraints = list(model._meta.constraints)\n \n+        # Filter out constraints with prohibited expressions in SQLite\n+        filtered_constraints = []\n+        for constraint in constraints:\n+            if isinstance(constraint, UniqueConstraint) and constraint.contains_expressions:\n+                continue  # Skip constraints with expressions containing prohibited symbols\n+            filtered_constraints.append(constraint)\n+        constraints = filtered_constraints\n+\n         # Provide isolated instances of the fields to the new model body so\n         # that the existing model's internals aren't interfered with when\n         # the dummy model is constructed.\n@@ -298,6 +306,12 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n             disable_constraints=False,\n         )\n \n+        # Reapply constraints that were skipped earlier\n+        for constraint in model._meta.constraints:\n+            if isinstance(constraint, UniqueConstraint) and constraint.contains_expressions:\n+                continue  # Skip constraints with expressions containing prohibited symbols\n+            self.add_constraint(new_model, constraint)\n+\n         # Run deferred SQL on correct table\n         for sql in self.deferred_sql:\n             self.execute(sql)\n@@ -427,7 +441,8 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n         ):\n             super().add_constraint(model, constraint)\n         else:\n-            self._remake_table(model)\n+            if not model._meta.db_table.startswith('new__'):\n+                self._remake_table(model)\n \n     def remove_constraint(self, model, constraint):\n         if isinstance(constraint, UniqueConstraint) and (\n@@ -438,7 +453,8 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n         ):\n             super().remove_constraint(model, constraint)\n         else:\n-            self._remake_table(model)\n+            if not model._meta.db_table.startswith('new__'):\n+                self._remake_table(model)\n \n     def _collate_sql(self, collation):\n         return 'COLLATE ' + collation", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "matplotlib__matplotlib-18869", "model_patch": "diff --git a/lib/matplotlib/__init__.py b/lib/matplotlib/__init__.py\n--- a/lib/matplotlib/__init__.py\n+++ b/lib/matplotlib/__init__.py\n@@ -173,6 +173,9 @@ def _check_versions():\n _check_versions()\n \n \n+# Expose version_info for version comparisons\n+version_info = None\n+\n # The decorator ensures this always returns the same handler (and it is only\n # attached once).\n @functools.lru_cache()", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "matplotlib__matplotlib-25311", "model_patch": "diff --git a/lib/matplotlib/legend.py b/lib/matplotlib/legend.py\n--- a/lib/matplotlib/legend.py\n+++ b/lib/matplotlib/legend.py\n@@ -331,6 +331,22 @@ class Legend(Artist):\n     Place a legend on the axes at location loc.\n     \"\"\"\n \n+    def __reduce__(self):\n+        \"\"\"Control how the legend is reduced (pickled).\"\"\"\n+        draggable = self._draggable is not None\n+        if draggable:\n+            self._draggable.disconnect()\n+        state = super().__reduce__()\n+        self._draggable = DraggableLegend(self, update=self._draggable._update) if draggable else None\n+        return state[0], state[1], state[2] + (draggable,)\n+\n+    def __setstate__(self, state):\n+        \"\"\"Control how the legend is restored (unpickled).\"\"\"\n+        *_, draggable = state\n+        super().__setstate__(state)\n+        if draggable:\n+            self.set_draggable(True, update=self._draggable._update if self._draggable else 'loc')\n+\n     # 'best' is only implemented for axes legends\n     codes = {'best': 0, **AnchoredOffsetbox.codes}\n     zorder = 5", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "matplotlib__matplotlib-25332", "model_patch": "diff --git a/lib/matplotlib/figure.py b/lib/matplotlib/figure.py\n--- a/lib/matplotlib/figure.py\n+++ b/lib/matplotlib/figure.py\n@@ -1376,22 +1376,18 @@ default: %(va)s\n         if axs is None:\n             axs = self.axes\n         axs = [ax for ax in np.ravel(axs) if ax.get_subplotspec() is not None]\n-        for ax in axs:\n+        from itertools import product\n+        group_pairs = list(product(axs, repeat=2))\n+        for ax, axc in group_pairs:\n             _log.debug(' Working on: %s', ax.get_xlabel())\n-            rowspan = ax.get_subplotspec().rowspan\n             pos = ax.xaxis.get_label_position()  # top or bottom\n-            # Search through other axes for label positions that are same as\n-            # this one and that share the appropriate row number.\n-            # Add to a grouper associated with each axes of siblings.\n-            # This list is inspected in `axis.draw` by\n-            # `axis._update_label_position`.\n-            for axc in axs:\n-                if axc.xaxis.get_label_position() == pos:\n-                    rowspanc = axc.get_subplotspec().rowspan\n-                    if (pos == 'top' and rowspan.start == rowspanc.start or\n-                            pos == 'bottom' and rowspan.stop == rowspanc.stop):\n-                        # grouper for groups of xlabels to align\n-                        self._align_label_groups['x'].join(ax, axc)\n+            if axc.xaxis.get_label_position() == pos:\n+                rowspan = ax.get_subplotspec().rowspan\n+                rowspanc = axc.get_subplotspec().rowspan\n+                if (pos == 'top' and rowspan.start == rowspanc.start or\n+                        pos == 'bottom' and rowspan.stop == rowspanc.stop):\n+                    # grouper for groups of xlabels to align\n+                    self._align_label_groups['x'].join(ax, axc)\n \n     def align_ylabels(self, axs=None):\n         \"\"\"\n@@ -1436,22 +1432,18 @@ default: %(va)s\n         if axs is None:\n             axs = self.axes\n         axs = [ax for ax in np.ravel(axs) if ax.get_subplotspec() is not None]\n-        for ax in axs:\n+        from itertools import product\n+        group_pairs = list(product(axs, repeat=2))\n+        for ax, axc in group_pairs:\n             _log.debug(' Working on: %s', ax.get_ylabel())\n-            colspan = ax.get_subplotspec().colspan\n             pos = ax.yaxis.get_label_position()  # left or right\n-            # Search through other axes for label positions that are same as\n-            # this one and that share the appropriate column number.\n-            # Add to a list associated with each axes of siblings.\n-            # This list is inspected in `axis.draw` by\n-            # `axis._update_label_position`.\n-            for axc in axs:\n-                if axc.yaxis.get_label_position() == pos:\n-                    colspanc = axc.get_subplotspec().colspan\n-                    if (pos == 'left' and colspan.start == colspanc.start or\n-                            pos == 'right' and colspan.stop == colspanc.stop):\n-                        # grouper for groups of ylabels to align\n-                        self._align_label_groups['y'].join(ax, axc)\n+            if axc.yaxis.get_label_position() == pos:\n+                colspan = ax.get_subplotspec().colspan\n+                colspanc = axc.get_subplotspec().colspan\n+                if (pos == 'left' and colspan.start == colspanc.start or\n+                        pos == 'right' and colspan.stop == colspanc.stop):\n+                    # grouper for groups of ylabels to align\n+                    self._align_label_groups['y'].join(ax, axc)\n \n     def align_labels(self, axs=None):\n         \"\"\"\n@@ -3179,17 +3171,18 @@ None}, default: None\n     def __setstate__(self, state):\n         version = state.pop('__mpl_version__')\n         restore_to_pylab = state.pop('_restore_to_pylab', False)\n-\n+    \n         if version != mpl.__version__:\n             _api.warn_external(\n                 f\"This figure was saved with matplotlib version {version} and \"\n-                f\"is unlikely to function correctly.\")\n-\n+                f\"is unlikely to function correctly.  Expect errors and incorrect \"\n+                f\"rendering.\")\n+    \n         self.__dict__ = state\n-\n+    \n         # re-initialise some of the unstored state information\n         FigureCanvasBase(self)  # Set self.canvas.\n-\n+    \n         if restore_to_pylab:\n             # lazy import to avoid circularity\n             import matplotlib.pyplot as plt\n@@ -3200,7 +3193,7 @@ None}, default: None\n             mgr = backend.new_figure_manager_given_figure(num, self)\n             pylab_helpers.Gcf._set_new_active_manager(mgr)\n             plt.draw_if_interactive()\n-\n+    \n         self.stale = True\n \n     def add_axobserver(self, func):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pallets__flask-5063", "model_patch": "diff --git a/src/flask/cli.py b/src/flask/cli.py\n--- a/src/flask/cli.py\n+++ b/src/flask/cli.py\n@@ -1018,20 +1018,34 @@ def routes_command(sort: str, all_methods: bool) -> None:\n         for rule in rules\n     ]\n \n-    headers = (\"Endpoint\", \"Methods\", \"Rule\")\n+    # Extract subdomain information\n+    def get_subdomain(rule):\n+        # Check if the blueprint has a subdomain attribute\n+        if rule.endpoint and \".\" in rule.endpoint:\n+            blueprint_name = rule.endpoint.split(\".\")[0]\n+            blueprint = current_app.blueprints.get(blueprint_name)\n+            if blueprint and hasattr(blueprint, \"subdomain\"):\n+                return blueprint.subdomain\n+        # Fall back to None if no subdomain is found\n+        return None\n+\n+    rule_subdomains = [get_subdomain(rule) for rule in rules]\n+\n+    headers = (\"Endpoint\", \"Methods\", \"Rule\", \"Domain\")\n     widths = (\n         max(len(rule.endpoint) for rule in rules),\n         max(len(methods) for methods in rule_methods),\n         max(len(rule.rule) for rule in rules),\n+        max(len(subdomain) if subdomain else 0 for subdomain in rule_subdomains),\n     )\n     widths = [max(len(h), w) for h, w in zip(headers, widths)]\n-    row = \"{{0:<{0}}}  {{1:<{1}}}  {{2:<{2}}}\".format(*widths)\n+    row = \"{{0:<{0}}}  {{1:<{1}}}  {{2:<{2}}}  {{3:<{3}}}\".format(*widths)\n \n     click.echo(row.format(*headers).strip())\n     click.echo(row.format(*(\"-\" * width for width in widths)))\n \n-    for rule, methods in zip(rules, rule_methods):\n-        click.echo(row.format(rule.endpoint, methods, rule.rule).rstrip())\n+    for rule, methods, subdomain in zip(rules, rule_methods, rule_subdomains):\n+        click.echo(row.format(rule.endpoint, methods, rule.rule, subdomain or \"\").rstrip())\n \n \n cli = FlaskGroup(", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pydata__xarray-3364", "model_patch": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -2,7 +2,7 @@ import pandas as pd\n \n from . import dtypes, utils\n from .alignment import align\n-from .merge import _VALID_COMPAT, unique_variable\n+from .merge import _VALID_COMPAT, MergeError, unique_variable\n from .variable import IndexVariable, Variable, as_variable\n from .variable import concat as concat_vars\n \n@@ -158,6 +158,7 @@ def _calc_concat_dim_coord(dim):\n def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n     \"\"\"\n     Determine which dataset variables need to be concatenated in the result,\n+    and handle missing variables by filling them with `NaN`.\n     \"\"\"\n     # Return values\n     concat_over = set()\n@@ -170,14 +171,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over_existing_dim = False\n \n     concat_dim_lengths = []\n+    all_vars = set()\n     for ds in datasets:\n         if concat_over_existing_dim:\n             if dim not in ds.dims:\n                 if dim in ds:\n                     ds = ds.set_coords(dim)\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n+        all_vars.update(ds.variables.keys())\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+    # Identify missing variables and fill with NaN\n+    for ds in datasets:\n+        missing_vars = all_vars - ds.variables.keys()\n+        for var in missing_vars:\n+            ds[var] = ds[dim] * float('nan')\n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -271,7 +280,8 @@ def _dataset_concat(\n     join=\"outer\",\n ):\n     \"\"\"\n-    Concatenate a sequence of datasets along a new or existing dimension\n+    Concatenate a sequence of datasets along a new or existing dimension,\n+    handling missing variables by filling them with `NaN`.\n     \"\"\"\n     from .dataset import Dataset\n \n@@ -307,6 +317,104 @@ def _dataset_concat(\n     # determine which variables to merge, and then merge them according to compat\n     variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n \n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            # Identify missing variables and fill with NaN\n+            for var in variables_to_merge:\n+                if var not in ds.variables:\n+                    ds[var] = ds[dim] * float('nan')\n+                to_merge[var].append(ds.variables[var])\n+\n+        for var in variables_to_merge:\n+            result_vars[var] = unique_variable(\n+                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n+            )\n+    else:\n+        result_vars = {}\n+    result_vars.update(dim_coords)\n+\n+    # assign attrs and encoding from first dataset\n+    result_attrs = datasets[0].attrs\n+    result_encoding = datasets[0].encoding\n+\n+    # check that global attributes are fixed across all datasets if necessary\n+    for ds in datasets[1:]:\n+        if compat == \"identical\" and not utils.dict_equiv(ds.attrs, result_attrs):\n+            raise ValueError(\"Dataset global attributes not equal.\")\n+\n+    # we've already verified everything is consistent; now, calculate\n+    # shared dimension sizes so we can expand the necessary variables\n+    def ensure_common_dims(vars):\n+        # ensure each variable with the given name shares the same\n+        # dimensions and the same shape for all of them except along the\n+        # concat dimension\n+        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n+        if dim not in common_dims:\n+            common_dims = (dim,) + common_dims\n+        for var, dim_len in zip(vars, concat_dim_lengths):\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+            except KeyError:\n+                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    absent_coord_names = coord_names - set(result.variables)\n+    if absent_coord_names:\n+        raise ValueError(\n+            \"Variables %r are coordinates in some datasets but not others.\"\n+            % absent_coord_names\n+        )\n+    result = result.set_coords(coord_names)\n+    result.encoding = result_encoding\n+\n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n+\n+    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n+    dim_names = set(dim_coords)\n+    unlabeled_dims = dim_names - coord_names\n+\n+    both_data_and_coords = coord_names & data_names\n+    if both_data_and_coords:\n+        raise ValueError(\n+            \"%r is a coordinate in some datasets but not others.\" % both_data_and_coords\n+        )\n+    # we don't want the concat dimension in the result dataset yet\n+    dim_coords.pop(dim, None)\n+    dims_sizes.pop(dim, None)\n+\n+    # case where concat dimension is a coordinate or data_var but not a dimension\n+    if (dim in coord_names or dim in data_names) and dim not in dim_names:\n+        datasets = [ds.expand_dims(dim) for ds in datasets]\n+\n+    # determine which variables to concatentate\n+    concat_over, equals, concat_dim_lengths = _calc_concat_over(\n+        datasets, dim, dim_names, data_vars, coords, compat\n+    )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n     result_vars = {}\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n@@ -408,7 +516,7 @@ def _dataarray_concat(\n             name = arr.name\n         elif name != arr.name:\n             if compat == \"identical\":\n-                raise ValueError(\"array names not identical\")\n+                raise MergeError(\"array names not identical\")\n             else:\n                 arr = arr.rename(name)\n         datasets.append(arr._to_temp_dataset())", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pydata__xarray-4493", "model_patch": "diff --git a/xarray/core/merge.py b/xarray/core/merge.py\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -910,7 +910,7 @@ def dataset_update_method(\n ) -> _MergeResult:\n     \"\"\"Guts of the Dataset.update method.\n \n-    This drops a duplicated coordinates from `other` if `other` is not an\n+    This drops duplicated coordinates from `other` if `other` is not an\n     `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n     GH2180).\n     \"\"\"\n@@ -930,6 +930,11 @@ def dataset_update_method(\n                 if coord_names:\n                     other[key] = value.drop_vars(coord_names)\n \n+                # Preserve chunking information for Dask DataArray\n+                if hasattr(value.data, 'chunks'):\n+                    chunks = value.data.chunks\n+                    other[key] = value.chunk(chunks)\n+\n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n     return merge_core(", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pylint-dev__pylint-7080", "model_patch": "diff --git a/pylint/pyreverse/utils.py b/pylint/pyreverse/utils.py\n--- a/pylint/pyreverse/utils.py\n+++ b/pylint/pyreverse/utils.py\n@@ -11,7 +11,8 @@ import re\n import shutil\n import subprocess\n import sys\n-from typing import TYPE_CHECKING, Any, Callable, Optional, Tuple, Union\n+import toml\n+from typing import TYPE_CHECKING, Any, Callable, Optional, Tuple, Union, List\n \n import astroid\n from astroid import nodes\n@@ -29,7 +30,7 @@ if TYPE_CHECKING:\n RCFILE = \".pyreverserc\"\n \n \n-def get_default_options() -> list[str]:\n+def get_default_options() -> List[str]:\n     \"\"\"Read config file and return list of options.\"\"\"\n     options = []\n     home = os.environ.get(\"HOME\", \"\")\n@@ -43,12 +44,32 @@ def get_default_options() -> list[str]:\n     return options\n \n \n-def insert_default_options() -> None:\n-    \"\"\"Insert default options to sys.argv.\"\"\"\n-    options = get_default_options()\n-    options.reverse()\n-    for arg in options:\n-        sys.argv.insert(1, arg)\n+def get_ignore_patterns(pyproject_path: str = \"pyproject.toml\") -> List[str]:\n+    \"\"\"Get ignore patterns from pyproject.toml.\"\"\"\n+    if not os.path.exists(pyproject_path):\n+        return []\n+\n+    with open(pyproject_path, encoding=\"utf-8\") as f:\n+        config = toml.load(f)\n+\n+    return config.get(\"tool\", {}).get(\"pylint\", {}).get(\"ignore-paths\", [])\n+\n+\n+def path_matches_ignore_patterns(path: str, patterns: List[str]) -> bool:\n+    \"\"\"Check if the given path matches any of the ignore patterns.\"\"\"\n+    return any(re.search(pattern, path) for pattern in patterns)\n+\n+\n+def recursive_file_list(directory: str) -> List[str]:\n+    \"\"\"Recursively list files in a directory, respecting ignore patterns.\"\"\"\n+    ignore_patterns = get_ignore_patterns()\n+    file_list = []\n+    for root, _, files in os.walk(directory):\n+        for file in files:\n+            file_path = os.path.join(root, file)\n+            if not path_matches_ignore_patterns(file_path, ignore_patterns):\n+                file_list.append(file_path)\n+    return file_list\n \n \n # astroid utilities ###########################################################\n@@ -57,20 +78,6 @@ PRIVATE = re.compile(r\"^__(_*[^\\W_])+_?$\")\n PROTECTED = re.compile(r\"^_\\w*$\")\n \n \n-def get_visibility(name: str) -> str:\n-    \"\"\"Return the visibility from a name: public, protected, private or special.\"\"\"\n-    if SPECIAL.match(name):\n-        visibility = \"special\"\n-    elif PRIVATE.match(name):\n-        visibility = \"private\"\n-    elif PROTECTED.match(name):\n-        visibility = \"protected\"\n-\n-    else:\n-        visibility = \"public\"\n-    return visibility\n-\n-\n def is_interface(node: nodes.ClassDef) -> bool:\n     # bw compatibility\n     return node.type == \"interface\"", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pylint-dev__pylint-7993", "model_patch": "diff --git a/pylint/reporters/text.py b/pylint/reporters/text.py\n--- a/pylint/reporters/text.py\n+++ b/pylint/reporters/text.py\n@@ -175,14 +175,15 @@ class TextReporter(BaseReporter):\n         self._template = template\n \n         # Check to see if all parameters in the template are attributes of the Message\n-        arguments = re.findall(r\"\\{(.+?)(:.*)?\\}\", template)\n+        # Updated regex to handle nested and custom braces\n+        arguments = re.findall(r\"\\{([^{}]+?)(:.*)?\\}\", template)\n         for argument in arguments:\n             if argument[0] not in MESSAGE_FIELDS:\n                 warnings.warn(\n                     f\"Don't recognize the argument '{argument[0]}' in the --msg-template. \"\n                     \"Are you sure it is supported on the current version of pylint?\"\n                 )\n-                template = re.sub(r\"\\{\" + argument[0] + r\"(:.*?)?\\}\", \"\", template)\n+                template = re.sub(r\"\\{\" + re.escape(argument[0]) + r\"(:.*?)?\\}\", \"\", template)\n         self._fixed_template = template\n \n     def write_message(self, msg: Message) -> None:\n@@ -193,7 +194,15 @@ class TextReporter(BaseReporter):\n         for key in (\"end_line\", \"end_column\"):\n             self_dict[key] = self_dict[key] or \"\"\n \n-        self.writeln(self._fixed_template.format(**self_dict))\n+        # Ensure that nested braces are handled correctly\n+        formatted_message = self._fixed_template.format(**self_dict)\n+        while '{' in formatted_message and '}' in formatted_message:\n+            try:\n+                formatted_message = formatted_message.format(**self_dict)\n+            except KeyError:\n+                break\n+\n+        self.writeln(formatted_message)\n \n     def handle_message(self, msg: Message) -> None:\n         \"\"\"Manage message of different type and in the context of path.\"\"\"", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pytest-dev__pytest-5103", "model_patch": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -983,13 +983,35 @@ warn_explicit(\n                 arg_expls.append(keyword.arg + \"=\" + expl)\n             else:  # **args have `arg` keywords with an .arg of None\n                 arg_expls.append(\"**\" + expl)\n-\n-        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n-        new_call = ast.Call(new_func, new_args, new_kwargs)\n-        res = self.assign(new_call)\n-        res_expl = self.explanation_param(self.display(res))\n-        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n-        return res, outer_expl\n+                \n+        # Handle `all` and `any` specifically\n+        if isinstance(call.func, ast.Name) and call.func.id in {\"all\", \"any\"}:\n+            self.statements.append(ast.Expr(ast.Str(s=\"Beginning enhanced {} assertion\".format(call.func.id))))\n+            bool_op, _bool_op_expl = self.assert_enhanced_boolop(call.func.id, new_args[0])\n+            new_call = self.assign(bool_op)\n+        else:\n+            expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n+            new_call = ast.Call(new_func, new_args, new_kwargs)\n+            res = self.assign(new_call)\n+            res_expl = self.explanation_param(self.display(res))\n+            outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n+            return res, outer_expl\n+    \n+    def assert_enhanced_boolop(self, func_name, iterable):\n+        \"\"\"Handles enhanced assertions for `all` and `any` built-in functions.\"\"\"\n+        temp_var = self.variable()\n+        bool_op = None\n+        \n+        if func_name == \"all\":\n+            bool_op = ast.BoolOp(op=ast.And(), values=[\n+                elem for elem in iterable.elts\n+            ])\n+        elif func_name == \"any\":\n+            bool_op = ast.BoolOp(op=ast.Or(), values=[\n+                elem for elem in iterable.elts\n+            ])\n+\n+        return bool_op, self.explanation_param(self.display(bool_op))\n \n     def visit_Starred(self, starred):\n         # From Python 3.5, a Starred node can appear in a function call", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pytest-dev__pytest-9359", "model_patch": "diff --git a/src/_pytest/_code/source.py b/src/_pytest/_code/source.py\n--- a/src/_pytest/_code/source.py\n+++ b/src/_pytest/_code/source.py\n@@ -144,17 +144,26 @@ def deindent(lines: Iterable[str]) -> List[str]:\n \n \n def get_statement_startend2(lineno: int, node: ast.AST) -> Tuple[int, Optional[int]]:\n-    # Flatten all statements and except handlers into one lineno-list.\n-    # AST's line numbers start indexing at 1.\n+    import sys\n+    \n+    # Helper function to handle different AST structures based on Python version\n+    def add_linenos(values: List[int], node: ast.AST, finalbody_name: str, orelse_name: str) -> None:\n+        for x in ast.walk(node):\n+            if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n+                values.append(x.lineno - 1)\n+                for name in (finalbody_name, orelse_name):\n+                    val: Optional[List[ast.stmt]] = getattr(x, name, None)\n+                    if val:\n+                        values.append(val[0].lineno - 1 - 1)\n+\n     values: List[int] = []\n-    for x in ast.walk(node):\n-        if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n-            values.append(x.lineno - 1)\n-            for name in (\"finalbody\", \"orelse\"):\n-                val: Optional[List[ast.stmt]] = getattr(x, name, None)\n-                if val:\n-                    # Treat the finally/orelse part as its own statement.\n-                    values.append(val[0].lineno - 1 - 1)\n+    \n+    # Use version-specific logic\n+    if sys.version_info >= (3, 9):\n+        add_linenos(values, node, \"finalbody\", \"orelse\")\n+    else:\n+        add_linenos(values, node, \"finalbody\", \"orelse\")\n+\n     values.sort()\n     insert_index = bisect_right(values, lineno)\n     start = values[insert_index - 1]", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-10508", "model_patch": "diff --git a/sklearn/preprocessing/label.py b/sklearn/preprocessing/label.py\n--- a/sklearn/preprocessing/label.py\n+++ b/sklearn/preprocessing/label.py\n@@ -127,6 +127,9 @@ class LabelEncoder(BaseEstimator, TransformerMixin):\n         check_is_fitted(self, 'classes_')\n         y = column_or_1d(y, warn=True)\n \n+        if len(y) == 0:\n+            return np.array([], dtype=self.classes_.dtype)\n+\n         classes = np.unique(y)\n         if len(np.intersect1d(classes, self.classes_)) < len(classes):\n             diff = np.setdiff1d(classes, self.classes_)", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-15512", "model_patch": "diff --git a/sklearn/cluster/_affinity_propagation.py b/sklearn/cluster/_affinity_propagation.py\n--- a/sklearn/cluster/_affinity_propagation.py\n+++ b/sklearn/cluster/_affinity_propagation.py\n@@ -388,8 +388,17 @@ class AffinityPropagation(ClusterMixin, BaseEstimator):\n                 convergence_iter=self.convergence_iter, damping=self.damping,\n                 copy=self.copy, verbose=self.verbose, return_n_iter=True)\n \n+        if self.n_iter_ == self.max_iter and not np.any(self.labels_ >= 0):\n+            warnings.warn(\"Affinity propagation did not converge, this model \"\n+                          \"will not have any cluster centers.\", ConvergenceWarning)\n+            self.cluster_centers_indices_ = np.array([])\n+            self.labels_ = np.array([-1] * X.shape[0])\n+\n         if self.affinity != \"precomputed\":\n-            self.cluster_centers_ = X[self.cluster_centers_indices_].copy()\n+            if self.cluster_centers_indices_.size > 0:\n+                self.cluster_centers_ = X[self.cluster_centers_indices_].copy()\n+            else:\n+                self.cluster_centers_ = np.array([])\n \n         return self", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-25638", "model_patch": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -4,11 +4,11 @@\n \"\"\"\n Multi-class / multi-label utility function\n ==========================================\n-\n \"\"\"\n from collections.abc import Sequence\n from itertools import chain\n import warnings\n+import pandas as pd\n \n from scipy.sparse import issparse\n from scipy.sparse import dok_matrix\n@@ -43,26 +43,21 @@ _FN_UNIQUE_LABELS = {\n \n def unique_labels(*ys):\n     \"\"\"Extract an ordered array of unique labels.\n-\n     We don't allow:\n         - mix of multilabel and multiclass (single label) targets\n         - mix of label indicator matrix and anything else,\n           because there are no explicit labels)\n         - mix of label indicator matrices of different sizes\n         - mix of string and integer labels\n-\n     At the moment, we also don't allow \"multiclass-multioutput\" input type.\n-\n     Parameters\n     ----------\n     *ys : array-likes\n         Label values.\n-\n     Returns\n     -------\n     out : ndarray of shape (n_unique_labels,)\n         An ordered array of unique labels.\n-\n     Examples\n     --------\n     >>> from sklearn.utils.multiclass import unique_labels\n@@ -73,9 +68,22 @@ def unique_labels(*ys):\n     >>> unique_labels([1, 2, 10], [5, 11])\n     array([ 1,  2,  5, 10, 11])\n     \"\"\"\n+    import pandas as pd\n+\n+    def convert_nullable_dtype(y):\n+        if pd.api.types.is_integer_dtype(y):\n+            return y.to_numpy(dtype=\"int64\")\n+        elif pd.api.types.is_float_dtype(y):\n+            return y.to_numpy(dtype=\"float64\")\n+        elif pd.api.types.is_bool_dtype(y):\n+            return y.to_numpy(dtype=\"bool\")\n+        return y\n+\n     xp, is_array_api = get_namespace(*ys)\n     if not ys:\n         raise ValueError(\"No argument has been passed.\")\n+    # Convert nullable dtypes to their numpy equivalents\n+    ys = [convert_nullable_dtype(y) for y in ys]\n     # Check that we don't mix label format\n \n     ys_types = set(type_of_target(x) for x in ys)", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-25747", "model_patch": "diff --git a/sklearn/utils/_set_output.py b/sklearn/utils/_set_output.py\n--- a/sklearn/utils/_set_output.py\n+++ b/sklearn/utils/_set_output.py\n@@ -96,6 +96,8 @@ def _get_output_config(method, estimator=None):\n     return {\"dense\": dense_config}\n \n \n+import warnings\n+\n def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):\n     \"\"\"Wrap output with container based on an estimator's or global config.\n \n@@ -127,9 +129,18 @@ def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):\n         return data_to_wrap\n \n     # dense_config == \"pandas\"\n+    index = getattr(original_input, \"index\", None)\n+    if index is not None and len(data_to_wrap) != len(index):\n+        warnings.warn(\n+            \"Length mismatch: Expected axis length does not match the number of rows in transformed data. \"\n+            \"Index will not be set.\",\n+            UserWarning,\n+        )\n+        index = None\n+\n     return _wrap_in_pandas_container(\n         data_to_wrap=data_to_wrap,\n-        index=getattr(original_input, \"index\", None),\n+        index=index,\n         columns=estimator.get_feature_names_out,\n     )", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sphinx-doc__sphinx-11445", "model_patch": "diff --git a/sphinx/util/rst.py b/sphinx/util/rst.py\n--- a/sphinx/util/rst.py\n+++ b/sphinx/util/rst.py\n@@ -82,6 +82,9 @@ def prepend_prolog(content: StringList, prolog: str) -> None:\n         for line in content:\n             if docinfo_re.match(line):\n                 pos += 1\n+            elif line.startswith(':') and '::' not in line:\n+                # Handle domain directives as first heading\n+                break\n             else:\n                 break\n \n@@ -96,6 +99,12 @@ def prepend_prolog(content: StringList, prolog: str) -> None:\n \n         content.insert(pos + lineno + 1, '', '<generated>', 0)\n \n+        # Ensure the first heading is treated correctly when it's a domain directive\n+        if content[pos].startswith(':') and '::' not in content[pos]:\n+            if ' ' in content[pos]:\n+                domain_directive_heading = content[pos].split(' ', 1)[1]\n+                content[pos] = f'{domain_directive_heading}\\n{\"=\" * len(domain_directive_heading)}'\n+\n \n def append_epilog(content: StringList, epilog: str) -> None:\n     \"\"\"Append a string to content body as epilog.\"\"\"", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sphinx-doc__sphinx-8273", "model_patch": "diff --git a/sphinx/builders/manpage.py b/sphinx/builders/manpage.py\n--- a/sphinx/builders/manpage.py\n+++ b/sphinx/builders/manpage.py\n@@ -80,10 +80,14 @@ class ManualPageBuilder(Builder):\n             docsettings.authors = authors\n             docsettings.section = section\n \n-            targetname = '%s.%s' % (name, section)\n+            import os  # Import os module\n+            section_dir = path.join(self.outdir, f'man{section}')\n+            targetname = path.join(section_dir, f'{name}.{section}')\n             logger.info(darkgreen(targetname) + ' { ', nonl=True)\n+            if not path.exists(section_dir):\n+                os.makedirs(section_dir)\n             destination = FileOutput(\n-                destination_path=path.join(self.outdir, targetname),\n+                destination_path=targetname,\n                 encoding='utf-8')\n \n             tree = self.env.get_doctree(docname)", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-14317", "model_patch": "diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -14,6 +14,7 @@\n from sympy.core.containers import Tuple\n from sympy.logic.boolalg import true\n from sympy.core.function import UndefinedFunction, AppliedUndef\n+from collections.abc import Mapping, Iterable, MutableSet, Callable\n \n ## sympy.printing imports\n from sympy.printing.precedence import precedence_traditional\n@@ -1853,8 +1854,16 @@ def _print_PolynomialRingBase(self, expr):\n         return r\"%s%s\\left[%s\\right]\" % (inv, domain, symbols)\n \n     def _print_Poly(self, poly):\n+        from sympy import Mul, S  # Add required imports\n         cls = poly.__class__.__name__\n-        expr = self._print(poly.as_expr())\n+        \n+        # Sort the monomials by their degree in descending order\n+        monomials = poly.terms()\n+        sorted_monomials = sorted(monomials, key=lambda term: sum(term[0]), reverse=True)\n+        sorted_poly_expr = sum([(coeff * Mul(*[gen**deg for gen, deg in zip(poly.gens, degrees)])) \n+                               for degrees, coeff in sorted_monomials], S.Zero)\n+\n+        expr = self._print(sorted_poly_expr)\n         gens = list(map(self._print, poly.gens))\n         domain = \"domain=%s\" % self._print(poly.get_domain())", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-14396", "model_patch": "diff --git a/sympy/polys/polyoptions.py b/sympy/polys/polyoptions.py\n--- a/sympy/polys/polyoptions.py\n+++ b/sympy/polys/polyoptions.py\n@@ -408,6 +408,7 @@ class Domain(with_metaclass(OptionType, Option)):\n     _re_polynomial = re.compile(r\"^(Z|ZZ|Q|QQ)\\[(.+)\\]$\")\n     _re_fraction = re.compile(r\"^(Z|ZZ|Q|QQ)\\((.+)\\)$\")\n     _re_algebraic = re.compile(r\"^(Q|QQ)\\<(.+)\\>$\")\n+    _re_realpoly = re.compile(r\"^(RR)\\[(.+)\\]$\")  # new regex pattern for RR[gens]\n \n     @classmethod\n     def preprocess(cls, domain):\n@@ -480,6 +481,12 @@ def preprocess(cls, domain):\n                 gens = list(map(sympify, r.groups()[1].split(',')))\n                 return sympy.polys.domains.QQ.algebraic_field(*gens)\n \n+            r = cls._re_realpoly.match(domain)  # new matching logic for RR[gens]\n+\n+            if r is not None:\n+                gens = list(map(sympify, r.groups()[1].split(',')))\n+                return sympy.polys.domains.RR.poly_ring(*gens)\n+\n         raise OptionError('expected a valid domain specification, got %s' % domain)\n \n     @classmethod", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-16792", "model_patch": "diff --git a/sympy/utilities/autowrap.py b/sympy/utilities/autowrap.py\n--- a/sympy/utilities/autowrap.py\n+++ b/sympy/utilities/autowrap.py\n@@ -455,15 +455,18 @@ def _declare_arg(self, arg):\n             shape = '(' + ','.join(self._string_var(i[1] + 1) for i in arg.dimensions) + ')'\n             return proto + \" = np.empty({shape})\".format(shape=shape)\n         else:\n-            return proto + \" = 0\"\n+            # Ensure scalar arguments are given appropriate initial values\n+            return proto + \" = 0.0\" if proto.startswith('double') else proto + \" = 0\"\n \n     def _call_arg(self, arg):\n         if arg.dimensions:\n             t = arg.get_datatype('c')\n             return \"<{0}*> {1}.data\".format(t, self._string_var(arg.name))\n-        elif isinstance(arg, ResultBase):\n+        elif isinstance(arg, (ResultBase, OutputArgument)):\n             return \"&{0}\".format(self._string_var(arg.name))\n         else:\n+            if isinstance(arg, InputArgument) and len(arg.dimensions) > 0 and arg.get_datatype('c').startswith('double'):\n+                return \"<{0}*> &{1}\".format(t, self._string_var(arg.name))    \n             return self._string_var(arg.name)\n \n     def _string_var(self, var):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-16988", "model_patch": "diff --git a/sympy/sets/handlers/intersection.py b/sympy/sets/handlers/intersection.py\n--- a/sympy/sets/handlers/intersection.py\n+++ b/sympy/sets/handlers/intersection.py\n@@ -419,14 +419,17 @@ def intersection_sets(a, b):\n \n @dispatch(FiniteSet, FiniteSet)\n def intersection_sets(a, b):\n-    return FiniteSet(*(a._elements & b._elements))\n+    # Convert to sets to eliminate duplicates and compute intersection\n+    return FiniteSet(*(set(a) & set(b)))\n \n @dispatch(FiniteSet, Set)\n def intersection_sets(a, b):\n+    # Convert FiniteSet to set to eliminate duplicates\n+    unique_a = set(a)\n     try:\n-        return FiniteSet(*[el for el in a if el in b])\n+        return FiniteSet(*[el for el in unique_a if el in b])\n     except TypeError:\n-        return None  # could not evaluate `el in b` due to symbolic ranges.\n+        return None  # Could not evaluate `el in b` due to symbolic ranges.\n \n @dispatch(Set, Set)\n def intersection_sets(a, b):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-20590", "model_patch": "diff --git a/sympy/core/symbol.py b/sympy/core/symbol.py\n--- a/sympy/core/symbol.py\n+++ b/sympy/core/symbol.py\n@@ -197,6 +197,8 @@ class Symbol(AtomicExpr, Boolean):\n     True\n \n     \"\"\"\n+    \n+    __slots__ = ('name',)\n \n     is_comparable = False\n \n@@ -366,6 +368,8 @@ class Dummy(Symbol):\n     _Dummy_10\n \n     \"\"\"\n+    \n+    __slots__ = ('dummy_index',)\n \n     # In the rare event that a Dummy object needs to be recreated, both the\n     # `name` and `dummy_index` should be passed.  This is used by `srepr` for\n@@ -436,6 +440,8 @@ class Wild(Symbol):\n         need to return ``True`` in order for the Wild instance\n         to match the expression.\n \n+    __slots__ = ('exclude', 'properties')\n+\n     Examples\n     ========", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-21171", "model_patch": "diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -1968,10 +1968,12 @@ def _print_DiracDelta(self, expr, exp=None):\n             tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n         return tex\n \n-    def _print_SingularityFunction(self, expr):\n+    def _print_SingularityFunction(self, expr, exp=None):\n         shift = self._print(expr.args[0] - expr.args[1])\n         power = self._print(expr.args[2])\n         tex = r\"{\\left\\langle %s \\right\\rangle}^{%s}\" % (shift, power)\n+        if exp is not None:\n+            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n         return tex\n \n     def _print_Heaviside(self, expr, exp=None):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-22840", "model_patch": "diff --git a/sympy/simplify/cse_main.py b/sympy/simplify/cse_main.py\n--- a/sympy/simplify/cse_main.py\n+++ b/sympy/simplify/cse_main.py\n@@ -568,6 +568,7 @@ def tree_cse(exprs, symbols, opt_subs=None, order='canonical', ignore=()):\n     \"\"\"\n     from sympy.matrices.expressions import MatrixExpr, MatrixSymbol, MatMul, MatAdd\n     from sympy.polys.rootoftools import RootOf\n+    from sympy.tensor.indexed import Indexed\n \n     if opt_subs is None:\n         opt_subs = dict()\n@@ -586,6 +587,24 @@ def _find_repeated(expr):\n         if isinstance(expr, RootOf):\n             return\n \n+        # Handle Indexed elements separately\n+        if isinstance(expr, Indexed):\n+            base = expr.base\n+            indices = expr.indices\n+            if base in seen_subexp and all(idx in seen_subexp for idx in indices):\n+                for ign in ignore:\n+                    if ign in expr.free_symbols:\n+                        break\n+                else:\n+                    to_eliminate.add(expr)\n+                    return\n+            seen_subexp.add(expr)\n+            seen_subexp.update(indices)\n+            _find_repeated(base)\n+            for idx in indices:\n+                _find_repeated(idx)\n+            return\n+\n         if isinstance(expr, Basic) and (expr.is_Atom or expr.is_Order):\n             if expr.is_Symbol:\n                 excluded_symbols.add(expr)\n@@ -643,6 +662,21 @@ def _rebuild(expr):\n         if expr in opt_subs:\n             expr = opt_subs[expr]\n \n+        # Ensure we handle Indexed elements of MatrixSymbol appropriately\n+        if isinstance(expr, Indexed):\n+            base = _rebuild(expr.base)\n+            indices = tuple(_rebuild(idx) for idx in expr.indices)\n+            new_expr = Indexed(base, *indices)\n+            if orig_expr in to_eliminate:\n+                try:\n+                    sym = next(symbols)\n+                except StopIteration:\n+                    raise ValueError(\"Symbols iterator ran out of symbols.\")\n+                subs[orig_expr] = sym\n+                replacements.append((sym, new_expr))\n+                return sym\n+            return new_expr\n+\n         # If enabled, parse Muls and Adds arguments by order to ensure\n         # replacement order independent from hashes\n         if order != 'none':", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-24102", "model_patch": "diff --git a/sympy/parsing/mathematica.py b/sympy/parsing/mathematica.py\n--- a/sympy/parsing/mathematica.py\n+++ b/sympy/parsing/mathematica.py\n@@ -586,7 +586,7 @@ def parse(self, s):\n         \"##\": lambda: [\"SlotSequence\", \"1\"],\n     }\n \n-    _literal = r\"[A-Za-z][A-Za-z0-9]*\"\n+    _literal = r\"[A-Za-z\\u0370-\\u03FF\\u1F00-\\u1FFF][A-Za-z0-9\\u0370-\\u03FF\\u1F00-\\u1FFF]*\"\n     _number = r\"(?:[0-9]+(?:\\.[0-9]*)?|\\.[0-9]+)\"\n \n     _enclosure_open = [\"(\", \"[\", \"[[\", \"{\"]", "model_name_or_path": "AppMap Navie GPT 4o"}
