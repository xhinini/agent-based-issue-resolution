{"instance_id": "astropy__astropy-14365", "model_patch": "diff --git a/astropy/io/ascii/qdp.py b/astropy/io/ascii/qdp.py\n--- a/astropy/io/ascii/qdp.py\n+++ b/astropy/io/ascii/qdp.py\n@@ -68,7 +68,7 @@ def _line_type(line, delimiter=None):\n     _new_re = rf\"NO({sep}NO)+\"\n     _data_re = rf\"({_decimal_re}|NO|[-+]?nan)({sep}({_decimal_re}|NO|[-+]?nan))*)\"\n     _type_re = rf\"^\\s*((?P<command>{_command_re})|(?P<new>{_new_re})|(?P<data>{_data_re})?\\s*(\\!(?P<comment>.*))?\\s*$\"\n-    _line_type_re = re.compile(_type_re)\n+    _line_type_re = re.compile(_type_re, re.IGNORECASE)\n     line = line.strip()\n     if not line:\n         return \"comment\"\n@@ -306,7 +306,7 @@ def _get_tables_from_qdp_file(qdp_file, input_colnames=None, delimiter=None):\n \n             values = []\n             for v in line.split(delimiter):\n-                if v == \"NO\":\n+                if v.upper() == \"NO\":\n                     values.append(np.ma.masked)\n                 else:\n                     # Understand if number is int or float", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "astropy__astropy-6938", "model_patch": "diff --git a/astropy/io/fits/fitsrec.py b/astropy/io/fits/fitsrec.py\n--- a/astropy/io/fits/fitsrec.py\n+++ b/astropy/io/fits/fitsrec.py\n@@ -1261,7 +1261,7 @@ def _scale_back_ascii(self, col_idx, input_field, output_field):\n \n         # Replace exponent separator in floating point numbers\n         if 'D' in format:\n-            output_field.replace(encode_ascii('E'), encode_ascii('D'))\n+            output_field[:] = output_field.replace(b'E', b'D')\n \n \n def _get_recarray_field(array, key):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "astropy__astropy-7746", "model_patch": "diff --git a/astropy/wcs/src/astropy_wcs.c b/astropy/wcs/src/astropy_wcs.c\n--- a/astropy/wcs/src/astropy_wcs.c\n+++ b/astropy/wcs/src/astropy_wcs.c\n@@ -235,6 +235,80 @@ Wcs_all_pix2world(\n     goto exit;\n   }\n \n+  // Handle empty input\n+  if (PyArray_SIZE(pixcrd) == 0) {\n+    world = (PyArrayObject*)PyArray_SimpleNew(2, PyArray_DIMS(pixcrd), NPY_DOUBLE);\n+    if (world == NULL) {\n+        goto exit;\n+    }\n+    Py_XDECREF(pixcrd);\n+    return (PyObject*)world;\n+  }\n+\n+  world = (PyArrayObject*)PyArray_SimpleNew(2, PyArray_DIMS(pixcrd), NPY_DOUBLE);\n+  if (world == NULL) {\n+    goto exit;\n+  }\n+\n+  /* Make the call */\n+  Py_BEGIN_ALLOW_THREADS\n+  preoffset_array(pixcrd, origin);\n+  wcsprm_python2c(self->x.wcs);\n+  status = pipeline_all_pixel2world(&self->x,\n+                                    (unsigned int)PyArray_DIM(pixcrd, 0),\n+                                    (unsigned int)PyArray_DIM(pixcrd, 1),\n+                                    (double*)PyArray_DATA(pixcrd),\n+                                    (double*)PyArray_DATA(world));\n+  wcsprm_c2python(self->x.wcs);\n+  unoffset_array(pixcrd, origin);\n+  Py_END_ALLOW_THREADS\n+  /* unoffset_array(world, origin); */\n+\n+ exit:\n+  Py_XDECREF(pixcrd);\n+\n+  if (status == 0 || status == 8) {\n+    return (PyObject*)world;\n+  } else {\n+    Py_XDECREF(world);\n+    if (status == -1) {\n+      PyErr_SetString(\n+        PyExc_ValueError,\n+        \"Wrong number of dimensions in input array.  Expected 2.\");\n+      return NULL;\n+    } else {\n+      if (status == -1) {\n+        /* exception already set */\n+        return NULL;\n+      } else {\n+        wcserr_to_python_exc(self->x.err);\n+        return NULL;\n+      }\n+    }\n+  }\n+}\n+\n+  if (!PyArg_ParseTupleAndKeywords(\n+          args, kwds, \"Oi:all_pix2world\", (char **)keywords,\n+          &pixcrd_obj, &origin)) {\n+    return NULL;\n+  }\n+\n+  naxis = self->x.wcs->naxis;\n+\n+  pixcrd = (PyArrayObject*)PyArray_ContiguousFromAny(pixcrd_obj, NPY_DOUBLE, 2, 2);\n+  if (pixcrd == NULL) {\n+    return NULL;\n+  }\n+\n+  if (PyArray_DIM(pixcrd, 1) < naxis) {\n+    PyErr_Format(\n+      PyExc_RuntimeError,\n+      \"Input array must be 2-dimensional, where the second dimension >= %d\",\n+      naxis);\n+    goto exit;\n+  }\n+\n   world = (PyArrayObject*)PyArray_SimpleNew(2, PyArray_DIMS(pixcrd), NPY_DOUBLE);\n   if (world == NULL) {\n     goto exit;\ndiff --git a/astropy/wcs/wcs.py b/astropy/wcs/wcs.py\n--- a/astropy/wcs/wcs.py\n+++ b/astropy/wcs/wcs.py\n@@ -1349,13 +1349,18 @@ reduce these to 2 dimensions using the naxis kwarg.\n     def wcs_pix2world(self, *args, **kwargs):\n         if self.wcs is None:\n             raise ValueError(\"No basic WCS settings were created.\")\n+    \n+        # Check for empty arrays and return empty results if so\n+        if any(isinstance(arg, np.ndarray) and arg.size == 0 for arg in args):\n+            return tuple(np.empty_like(arg) for arg in args if isinstance(arg, np.ndarray))\n+    \n         return self._array_converter(\n             lambda xy, o: self.wcs.p2s(xy, o)['world'],\n             'output', *args, **kwargs)\n     wcs_pix2world.__doc__ = \"\"\"\n         Transforms pixel coordinates to world coordinates by doing\n         only the basic `wcslib`_ transformation.\n-\n+    \n         No `SIP`_ or `distortion paper`_ table lookup correction is\n         applied.  To perform distortion correction, see\n         `~astropy.wcs.WCS.all_pix2world`,\n@@ -2156,6 +2161,11 @@ reduce these to 2 dimensions using the naxis kwarg.\n     def wcs_world2pix(self, *args, **kwargs):\n         if self.wcs is None:\n             raise ValueError(\"No basic WCS settings were created.\")\n+    \n+        # Check for empty arrays and return empty results if so\n+        if any(isinstance(arg, np.ndarray) and arg.size == 0 for arg in args):\n+            return tuple(np.empty_like(arg) for arg in args if isinstance(arg, np.ndarray))\n+    \n         return self._array_converter(\n             lambda xy, o: self.wcs.s2p(xy, o)['pixcrd'],\n             'input', *args, **kwargs)\n@@ -2163,21 +2173,21 @@ reduce these to 2 dimensions using the naxis kwarg.\n         Transforms world coordinates to pixel coordinates, using only\n         the basic `wcslib`_ WCS transformation.  No `SIP`_ or\n         `distortion paper`_ table lookup transformation is applied.\n-\n+    \n         Parameters\n         ----------\n         {0}\n-\n+    \n             For a transformation that is not two-dimensional, the\n             two-argument form must be used.\n-\n+    \n         {1}\n-\n+    \n         Returns\n         -------\n-\n+    \n         {2}\n-\n+    \n         Notes\n         -----\n         The order of the axes for the input world array is determined by\n@@ -2186,30 +2196,30 @@ reduce these to 2 dimensions using the naxis kwarg.\n         `~astropy.wcs.Wcsprm.lat`, `~astropy.wcs.Wcsprm.lng`,\n         `~astropy.wcs.Wcsprm.lattyp` and `~astropy.wcs.Wcsprm.lngtyp`\n         members can be used to determine the order of the axes.\n-\n+    \n         Raises\n         ------\n         MemoryError\n             Memory allocation failed.\n-\n+    \n         SingularMatrixError\n             Linear transformation matrix is singular.\n-\n+    \n         InconsistentAxisTypesError\n             Inconsistent or unrecognized coordinate axis types.\n-\n+    \n         ValueError\n             Invalid parameter value.\n-\n+    \n         ValueError\n             Invalid coordinate transformation parameters.\n-\n+    \n         ValueError\n             x- and y-coordinate arrays are not the same size.\n-\n+    \n         InvalidTransformError\n             Invalid coordinate transformation parameters.\n-\n+    \n         InvalidTransformError\n             Ill-conditioned coordinate transformation parameters.\n         \"\"\".format(__.TWO_OR_MORE_ARGS('naxis', 8),", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-12589", "model_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -473,13 +473,18 @@ class Query(BaseExpression):\n                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)\n                     del inner_query.annotations[alias]\n                     annotation_select_mask.remove(alias)\n-                # Make sure the annotation_select wont use cached results.\n-                inner_query.set_annotation_mask(inner_query.annotation_select_mask)\n-            if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:\n-                # In case of Model.objects[0:3].count(), there would be no\n-                # field selected in the inner query, yet we must use a subquery.\n-                # So, make sure at least one field is selected.\n-                inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n+                # If alias is reused in select fields, add a distinct one\n+                    if alias in [column.alias for column in outer_query.select]:\n+                        new_alias = alias + '_subq'\n+                        outer_query.annotations[new_alias] = outer_query.annotations.pop(alias)\n+                        alias = new_alias\n+                    # Make sure the annotation_select wont use cached results.\n+                    inner_query.set_annotation_mask(inner_query.annotation_select_mask)\n+                if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:\n+                    # In case of Model.objects[0:3].count(), there would be no\n+                    # field selected in the inner query, yet we must use a subquery.\n+                    # So, make sure at least one field is selected.\n+                    inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n             try:\n                 outer_query.add_subquery(inner_query, using)\n             except EmptyResultSet:", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-12747", "model_patch": "diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -388,6 +388,10 @@ class Collector:\n         # number of objects deleted for each model label\n         deleted_counter = Counter()\n \n+        # Initialize deleted_counter with all involved model labels set to zero\n+        for model in self.data.keys():\n+            deleted_counter[model._meta.label] = 0\n+\n         # Optimize for the case with a single obj and no dependencies\n         if len(self.data) == 1 and len(instances) == 1:\n             instance = list(instances)[0]\n@@ -442,4 +446,10 @@ class Collector:\n         for model, instances in self.data.items():\n             for instance in instances:\n                 setattr(instance, model._meta.pk.attname, None)\n+\n+        # Ensure zero value for any model that did not have deletions\n+        for model in self.data.keys():\n+            if model._meta.label not in deleted_counter:\n+                deleted_counter[model._meta.label] = 0\n+\n         return sum(deleted_counter.values()), dict(deleted_counter)", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-14534", "model_patch": "diff --git a/django/forms/boundfield.py b/django/forms/boundfield.py\n--- a/django/forms/boundfield.py\n+++ b/django/forms/boundfield.py\n@@ -277,7 +277,7 @@ class BoundWidget:\n \n     @property\n     def id_for_label(self):\n-        return 'id_%s_%s' % (self.data['name'], self.data['index'])\n+        return self.data['attrs'].get('id', 'id_%s_%s' % (self.data['name'], self.data['index']))\n \n     @property\n     def choice_label(self):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "django__django-14997", "model_patch": "diff --git a/django/db/backends/sqlite3/schema.py b/django/db/backends/sqlite3/schema.py\n--- a/django/db/backends/sqlite3/schema.py\n+++ b/django/db/backends/sqlite3/schema.py\n@@ -238,6 +238,14 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n \n         constraints = list(model._meta.constraints)\n \n+        # Filter out constraints with prohibited expressions in SQLite\n+        filtered_constraints = []\n+        for constraint in constraints:\n+            if isinstance(constraint, UniqueConstraint) and constraint.contains_expressions:\n+                continue  # Skip constraints with expressions containing prohibited symbols\n+            filtered_constraints.append(constraint)\n+        constraints = filtered_constraints\n+\n         # Provide isolated instances of the fields to the new model body so\n         # that the existing model's internals aren't interfered with when\n         # the dummy model is constructed.\n@@ -298,6 +306,12 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n             disable_constraints=False,\n         )\n \n+        # Reapply constraints that were skipped earlier\n+        for constraint in model._meta.constraints:\n+            if isinstance(constraint, UniqueConstraint) and constraint.contains_expressions:\n+                continue  # Skip constraints with expressions containing prohibited symbols\n+            self.add_constraint(new_model, constraint)\n+\n         # Run deferred SQL on correct table\n         for sql in self.deferred_sql:\n             self.execute(sql)\n@@ -427,7 +441,8 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n         ):\n             super().add_constraint(model, constraint)\n         else:\n-            self._remake_table(model)\n+            if not model._meta.db_table.startswith('new__'):\n+                self._remake_table(model)\n \n     def remove_constraint(self, model, constraint):\n         if isinstance(constraint, UniqueConstraint) and (\n@@ -438,7 +453,8 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n         ):\n             super().remove_constraint(model, constraint)\n         else:\n-            self._remake_table(model)\n+            if not model._meta.db_table.startswith('new__'):\n+                self._remake_table(model)\n \n     def _collate_sql(self, collation):\n         return 'COLLATE ' + collation", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "matplotlib__matplotlib-18869", "model_patch": "diff --git a/lib/matplotlib/__init__.py b/lib/matplotlib/__init__.py\n--- a/lib/matplotlib/__init__.py\n+++ b/lib/matplotlib/__init__.py\n@@ -173,6 +173,9 @@ def _check_versions():\n _check_versions()\n \n \n+# Expose version_info for version comparisons\n+version_info = None\n+\n # The decorator ensures this always returns the same handler (and it is only\n # attached once).\n @functools.lru_cache()", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "matplotlib__matplotlib-25311", "model_patch": "diff --git a/lib/matplotlib/offsetbox.py b/lib/matplotlib/offsetbox.py\n--- a/lib/matplotlib/offsetbox.py\n+++ b/lib/matplotlib/offsetbox.py\n@@ -1505,7 +1505,6 @@ def __init__(self, ref_artist, use_blit=False):\n         if not ref_artist.pickable():\n             ref_artist.set_picker(True)\n         self.got_artist = False\n-        self.canvas = self.ref_artist.figure.canvas\n         self._use_blit = use_blit and self.canvas.supports_blit\n         self.cids = [\n             self.canvas.callbacks._connect_picklable(\n@@ -1514,6 +1513,9 @@ def __init__(self, ref_artist, use_blit=False):\n                 'button_release_event', self.on_release),\n         ]\n \n+    # Property to ensure proper picklability of the canvas.\n+    canvas = property(lambda self: self.ref_artist.figure.canvas)\n+\n     def on_motion(self, evt):\n         if self._check_still_parented() and self.got_artist:\n             dx = evt.x - self.mouse_x", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "matplotlib__matplotlib-25332", "model_patch": "diff --git a/lib/matplotlib/figure.py b/lib/matplotlib/figure.py\n--- a/lib/matplotlib/figure.py\n+++ b/lib/matplotlib/figure.py\n@@ -1376,22 +1376,18 @@ default: %(va)s\n         if axs is None:\n             axs = self.axes\n         axs = [ax for ax in np.ravel(axs) if ax.get_subplotspec() is not None]\n-        for ax in axs:\n+        from itertools import product\n+        group_pairs = list(product(axs, repeat=2))\n+        for ax, axc in group_pairs:\n             _log.debug(' Working on: %s', ax.get_xlabel())\n-            rowspan = ax.get_subplotspec().rowspan\n             pos = ax.xaxis.get_label_position()  # top or bottom\n-            # Search through other axes for label positions that are same as\n-            # this one and that share the appropriate row number.\n-            # Add to a grouper associated with each axes of siblings.\n-            # This list is inspected in `axis.draw` by\n-            # `axis._update_label_position`.\n-            for axc in axs:\n-                if axc.xaxis.get_label_position() == pos:\n-                    rowspanc = axc.get_subplotspec().rowspan\n-                    if (pos == 'top' and rowspan.start == rowspanc.start or\n-                            pos == 'bottom' and rowspan.stop == rowspanc.stop):\n-                        # grouper for groups of xlabels to align\n-                        self._align_label_groups['x'].join(ax, axc)\n+            if axc.xaxis.get_label_position() == pos:\n+                rowspan = ax.get_subplotspec().rowspan\n+                rowspanc = axc.get_subplotspec().rowspan\n+                if (pos == 'top' and rowspan.start == rowspanc.start or\n+                        pos == 'bottom' and rowspan.stop == rowspanc.stop):\n+                    # grouper for groups of xlabels to align\n+                    self._align_label_groups['x'].join(ax, axc)\n \n     def align_ylabels(self, axs=None):\n         \"\"\"\n@@ -1436,22 +1432,18 @@ default: %(va)s\n         if axs is None:\n             axs = self.axes\n         axs = [ax for ax in np.ravel(axs) if ax.get_subplotspec() is not None]\n-        for ax in axs:\n+        from itertools import product\n+        group_pairs = list(product(axs, repeat=2))\n+        for ax, axc in group_pairs:\n             _log.debug(' Working on: %s', ax.get_ylabel())\n-            colspan = ax.get_subplotspec().colspan\n             pos = ax.yaxis.get_label_position()  # left or right\n-            # Search through other axes for label positions that are same as\n-            # this one and that share the appropriate column number.\n-            # Add to a list associated with each axes of siblings.\n-            # This list is inspected in `axis.draw` by\n-            # `axis._update_label_position`.\n-            for axc in axs:\n-                if axc.yaxis.get_label_position() == pos:\n-                    colspanc = axc.get_subplotspec().colspan\n-                    if (pos == 'left' and colspan.start == colspanc.start or\n-                            pos == 'right' and colspan.stop == colspanc.stop):\n-                        # grouper for groups of ylabels to align\n-                        self._align_label_groups['y'].join(ax, axc)\n+            if axc.yaxis.get_label_position() == pos:\n+                colspan = ax.get_subplotspec().colspan\n+                colspanc = axc.get_subplotspec().colspan\n+                if (pos == 'left' and colspan.start == colspanc.start or\n+                        pos == 'right' and colspan.stop == colspanc.stop):\n+                    # grouper for groups of ylabels to align\n+                    self._align_label_groups['y'].join(ax, axc)\n \n     def align_labels(self, axs=None):\n         \"\"\"\n@@ -3179,17 +3171,18 @@ None}, default: None\n     def __setstate__(self, state):\n         version = state.pop('__mpl_version__')\n         restore_to_pylab = state.pop('_restore_to_pylab', False)\n-\n+    \n         if version != mpl.__version__:\n             _api.warn_external(\n                 f\"This figure was saved with matplotlib version {version} and \"\n-                f\"is unlikely to function correctly.\")\n-\n+                f\"is unlikely to function correctly.  Expect errors and incorrect \"\n+                f\"rendering.\")\n+    \n         self.__dict__ = state\n-\n+    \n         # re-initialise some of the unstored state information\n         FigureCanvasBase(self)  # Set self.canvas.\n-\n+    \n         if restore_to_pylab:\n             # lazy import to avoid circularity\n             import matplotlib.pyplot as plt\n@@ -3200,7 +3193,7 @@ None}, default: None\n             mgr = backend.new_figure_manager_given_figure(num, self)\n             pylab_helpers.Gcf._set_new_active_manager(mgr)\n             plt.draw_if_interactive()\n-\n+    \n         self.stale = True\n \n     def add_axobserver(self, func):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pallets__flask-5063", "model_patch": "diff --git a/src/flask/cli.py b/src/flask/cli.py\n--- a/src/flask/cli.py\n+++ b/src/flask/cli.py\n@@ -1018,20 +1018,34 @@ def routes_command(sort: str, all_methods: bool) -> None:\n         for rule in rules\n     ]\n \n-    headers = (\"Endpoint\", \"Methods\", \"Rule\")\n+    # Extract subdomain information\n+    def get_subdomain(rule):\n+        # Check if the blueprint has a subdomain attribute\n+        if rule.endpoint and \".\" in rule.endpoint:\n+            blueprint_name = rule.endpoint.split(\".\")[0]\n+            blueprint = current_app.blueprints.get(blueprint_name)\n+            if blueprint and hasattr(blueprint, \"subdomain\"):\n+                return blueprint.subdomain\n+        # Fall back to None if no subdomain is found\n+        return None\n+\n+    rule_subdomains = [get_subdomain(rule) for rule in rules]\n+\n+    headers = (\"Endpoint\", \"Methods\", \"Rule\", \"Domain\")\n     widths = (\n         max(len(rule.endpoint) for rule in rules),\n         max(len(methods) for methods in rule_methods),\n         max(len(rule.rule) for rule in rules),\n+        max(len(subdomain) if subdomain else 0 for subdomain in rule_subdomains),\n     )\n     widths = [max(len(h), w) for h, w in zip(headers, widths)]\n-    row = \"{{0:<{0}}}  {{1:<{1}}}  {{2:<{2}}}\".format(*widths)\n+    row = \"{{0:<{0}}}  {{1:<{1}}}  {{2:<{2}}}  {{3:<{3}}}\".format(*widths)\n \n     click.echo(row.format(*headers).strip())\n     click.echo(row.format(*(\"-\" * width for width in widths)))\n \n-    for rule, methods in zip(rules, rule_methods):\n-        click.echo(row.format(rule.endpoint, methods, rule.rule).rstrip())\n+    for rule, methods, subdomain in zip(rules, rule_methods, rule_subdomains):\n+        click.echo(row.format(rule.endpoint, methods, rule.rule, subdomain or \"\").rstrip())\n \n \n cli = FlaskGroup(", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pydata__xarray-3364", "model_patch": "diff --git a/xarray/core/concat.py b/xarray/core/concat.py\n--- a/xarray/core/concat.py\n+++ b/xarray/core/concat.py\n@@ -2,7 +2,7 @@ import pandas as pd\n \n from . import dtypes, utils\n from .alignment import align\n-from .merge import _VALID_COMPAT, unique_variable\n+from .merge import _VALID_COMPAT, MergeError, unique_variable\n from .variable import IndexVariable, Variable, as_variable\n from .variable import concat as concat_vars\n \n@@ -158,6 +158,7 @@ def _calc_concat_dim_coord(dim):\n def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n     \"\"\"\n     Determine which dataset variables need to be concatenated in the result,\n+    and handle missing variables by filling them with `NaN`.\n     \"\"\"\n     # Return values\n     concat_over = set()\n@@ -170,14 +171,22 @@ def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):\n         concat_over_existing_dim = False\n \n     concat_dim_lengths = []\n+    all_vars = set()\n     for ds in datasets:\n         if concat_over_existing_dim:\n             if dim not in ds.dims:\n                 if dim in ds:\n                     ds = ds.set_coords(dim)\n         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)\n+        all_vars.update(ds.variables.keys())\n         concat_dim_lengths.append(ds.dims.get(dim, 1))\n \n+    # Identify missing variables and fill with NaN\n+    for ds in datasets:\n+        missing_vars = all_vars - ds.variables.keys()\n+        for var in missing_vars:\n+            ds[var] = ds[dim] * float('nan')\n+\n     def process_subset_opt(opt, subset):\n         if isinstance(opt, str):\n             if opt == \"different\":\n@@ -271,7 +280,8 @@ def _dataset_concat(\n     join=\"outer\",\n ):\n     \"\"\"\n-    Concatenate a sequence of datasets along a new or existing dimension\n+    Concatenate a sequence of datasets along a new or existing dimension,\n+    handling missing variables by filling them with `NaN`.\n     \"\"\"\n     from .dataset import Dataset\n \n@@ -307,6 +317,104 @@ def _dataset_concat(\n     # determine which variables to merge, and then merge them according to compat\n     variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n \n+    result_vars = {}\n+    if variables_to_merge:\n+        to_merge = {var: [] for var in variables_to_merge}\n+\n+        for ds in datasets:\n+            # Identify missing variables and fill with NaN\n+            for var in variables_to_merge:\n+                if var not in ds.variables:\n+                    ds[var] = ds[dim] * float('nan')\n+                to_merge[var].append(ds.variables[var])\n+\n+        for var in variables_to_merge:\n+            result_vars[var] = unique_variable(\n+                var, to_merge[var], compat=compat, equals=equals.get(var, None)\n+            )\n+    else:\n+        result_vars = {}\n+    result_vars.update(dim_coords)\n+\n+    # assign attrs and encoding from first dataset\n+    result_attrs = datasets[0].attrs\n+    result_encoding = datasets[0].encoding\n+\n+    # check that global attributes are fixed across all datasets if necessary\n+    for ds in datasets[1:]:\n+        if compat == \"identical\" and not utils.dict_equiv(ds.attrs, result_attrs):\n+            raise ValueError(\"Dataset global attributes not equal.\")\n+\n+    # we've already verified everything is consistent; now, calculate\n+    # shared dimension sizes so we can expand the necessary variables\n+    def ensure_common_dims(vars):\n+        # ensure each variable with the given name shares the same\n+        # dimensions and the same shape for all of them except along the\n+        # concat dimension\n+        common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n+        if dim not in common_dims:\n+            common_dims = (dim,) + common_dims\n+        for var, dim_len in zip(vars, concat_dim_lengths):\n+            if var.dims != common_dims:\n+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)\n+                var = var.set_dims(common_dims, common_shape)\n+            yield var\n+\n+    # stack up each variable to fill-out the dataset (in order)\n+    # n.b. this loop preserves variable order, needed for groupby.\n+    for k in datasets[0].variables:\n+        if k in concat_over:\n+            try:\n+                vars = ensure_common_dims([ds.variables[k] for ds in datasets])\n+            except KeyError:\n+                raise ValueError(\"%r is not present in all datasets.\" % k)\n+            combined = concat_vars(vars, dim, positions)\n+            assert isinstance(combined, Variable)\n+            result_vars[k] = combined\n+\n+    result = Dataset(result_vars, attrs=result_attrs)\n+    absent_coord_names = coord_names - set(result.variables)\n+    if absent_coord_names:\n+        raise ValueError(\n+            \"Variables %r are coordinates in some datasets but not others.\"\n+            % absent_coord_names\n+        )\n+    result = result.set_coords(coord_names)\n+    result.encoding = result_encoding\n+\n+    result = result.drop(unlabeled_dims, errors=\"ignore\")\n+\n+    if coord is not None:\n+        # add concat dimension last to ensure that its in the final Dataset\n+        result[coord.name] = coord\n+\n+    return result\n+\n+    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)\n+    dim_names = set(dim_coords)\n+    unlabeled_dims = dim_names - coord_names\n+\n+    both_data_and_coords = coord_names & data_names\n+    if both_data_and_coords:\n+        raise ValueError(\n+            \"%r is a coordinate in some datasets but not others.\" % both_data_and_coords\n+        )\n+    # we don't want the concat dimension in the result dataset yet\n+    dim_coords.pop(dim, None)\n+    dims_sizes.pop(dim, None)\n+\n+    # case where concat dimension is a coordinate or data_var but not a dimension\n+    if (dim in coord_names or dim in data_names) and dim not in dim_names:\n+        datasets = [ds.expand_dims(dim) for ds in datasets]\n+\n+    # determine which variables to concatentate\n+    concat_over, equals, concat_dim_lengths = _calc_concat_over(\n+        datasets, dim, dim_names, data_vars, coords, compat\n+    )\n+\n+    # determine which variables to merge, and then merge them according to compat\n+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names\n+\n     result_vars = {}\n     if variables_to_merge:\n         to_merge = {var: [] for var in variables_to_merge}\n@@ -408,7 +516,7 @@ def _dataarray_concat(\n             name = arr.name\n         elif name != arr.name:\n             if compat == \"identical\":\n-                raise ValueError(\"array names not identical\")\n+                raise MergeError(\"array names not identical\")\n             else:\n                 arr = arr.rename(name)\n         datasets.append(arr._to_temp_dataset())", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pydata__xarray-4493", "model_patch": "diff --git a/xarray/core/merge.py b/xarray/core/merge.py\n--- a/xarray/core/merge.py\n+++ b/xarray/core/merge.py\n@@ -910,7 +910,7 @@ def dataset_update_method(\n ) -> _MergeResult:\n     \"\"\"Guts of the Dataset.update method.\n \n-    This drops a duplicated coordinates from `other` if `other` is not an\n+    This drops duplicated coordinates from `other` if `other` is not an\n     `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,\n     GH2180).\n     \"\"\"\n@@ -930,6 +930,11 @@ def dataset_update_method(\n                 if coord_names:\n                     other[key] = value.drop_vars(coord_names)\n \n+                # Preserve chunking information for Dask DataArray\n+                if hasattr(value.data, 'chunks'):\n+                    chunks = value.data.chunks\n+                    other[key] = value.chunk(chunks)\n+\n     # use ds.coords and not ds.indexes, else str coords are cast to object\n     indexes = {key: dataset.coords[key] for key in dataset.indexes.keys()}\n     return merge_core(", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pylint-dev__pylint-7080", "model_patch": "diff --git a/pylint/lint/expand_modules.py b/pylint/lint/expand_modules.py\n--- a/pylint/lint/expand_modules.py\n+++ b/pylint/lint/expand_modules.py\n@@ -52,6 +52,7 @@ def _is_ignored_file(\n     ignore_list_re: list[Pattern[str]],\n     ignore_list_paths_re: list[Pattern[str]],\n ) -> bool:\n+    element: str = os.path.normpath(element)\n     basename = os.path.basename(element)\n     return (\n         basename in ignore_list", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pylint-dev__pylint-7993", "model_patch": "diff --git a/pylint/reporters/text.py b/pylint/reporters/text.py\n--- a/pylint/reporters/text.py\n+++ b/pylint/reporters/text.py\n@@ -175,7 +175,7 @@ def on_set_current_module(self, module: str, filepath: str | None) -> None:\n         self._template = template\n \n         # Check to see if all parameters in the template are attributes of the Message\n-        arguments = re.findall(r\"\\{(.+?)(:.*)?\\}\", template)\n+        arguments = re.findall(r\"\\{(\\w+?)(:.*)?\\}\", template)\n         for argument in arguments:\n             if argument[0] not in MESSAGE_FIELDS:\n                 warnings.warn(", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pytest-dev__pytest-5103", "model_patch": "diff --git a/src/_pytest/assertion/rewrite.py b/src/_pytest/assertion/rewrite.py\n--- a/src/_pytest/assertion/rewrite.py\n+++ b/src/_pytest/assertion/rewrite.py\n@@ -983,13 +983,35 @@ warn_explicit(\n                 arg_expls.append(keyword.arg + \"=\" + expl)\n             else:  # **args have `arg` keywords with an .arg of None\n                 arg_expls.append(\"**\" + expl)\n-\n-        expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n-        new_call = ast.Call(new_func, new_args, new_kwargs)\n-        res = self.assign(new_call)\n-        res_expl = self.explanation_param(self.display(res))\n-        outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n-        return res, outer_expl\n+                \n+        # Handle `all` and `any` specifically\n+        if isinstance(call.func, ast.Name) and call.func.id in {\"all\", \"any\"}:\n+            self.statements.append(ast.Expr(ast.Str(s=\"Beginning enhanced {} assertion\".format(call.func.id))))\n+            bool_op, _bool_op_expl = self.assert_enhanced_boolop(call.func.id, new_args[0])\n+            new_call = self.assign(bool_op)\n+        else:\n+            expl = \"%s(%s)\" % (func_expl, \", \".join(arg_expls))\n+            new_call = ast.Call(new_func, new_args, new_kwargs)\n+            res = self.assign(new_call)\n+            res_expl = self.explanation_param(self.display(res))\n+            outer_expl = \"%s\\n{%s = %s\\n}\" % (res_expl, res_expl, expl)\n+            return res, outer_expl\n+    \n+    def assert_enhanced_boolop(self, func_name, iterable):\n+        \"\"\"Handles enhanced assertions for `all` and `any` built-in functions.\"\"\"\n+        temp_var = self.variable()\n+        bool_op = None\n+        \n+        if func_name == \"all\":\n+            bool_op = ast.BoolOp(op=ast.And(), values=[\n+                elem for elem in iterable.elts\n+            ])\n+        elif func_name == \"any\":\n+            bool_op = ast.BoolOp(op=ast.Or(), values=[\n+                elem for elem in iterable.elts\n+            ])\n+\n+        return bool_op, self.explanation_param(self.display(bool_op))\n \n     def visit_Starred(self, starred):\n         # From Python 3.5, a Starred node can appear in a function call", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "pytest-dev__pytest-9359", "model_patch": "diff --git a/src/_pytest/_code/source.py b/src/_pytest/_code/source.py\n--- a/src/_pytest/_code/source.py\n+++ b/src/_pytest/_code/source.py\n@@ -144,17 +144,26 @@ def deindent(lines: Iterable[str]) -> List[str]:\n \n \n def get_statement_startend2(lineno: int, node: ast.AST) -> Tuple[int, Optional[int]]:\n-    # Flatten all statements and except handlers into one lineno-list.\n-    # AST's line numbers start indexing at 1.\n+    import sys\n+    \n+    # Helper function to handle different AST structures based on Python version\n+    def add_linenos(values: List[int], node: ast.AST, finalbody_name: str, orelse_name: str) -> None:\n+        for x in ast.walk(node):\n+            if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n+                values.append(x.lineno - 1)\n+                for name in (finalbody_name, orelse_name):\n+                    val: Optional[List[ast.stmt]] = getattr(x, name, None)\n+                    if val:\n+                        values.append(val[0].lineno - 1 - 1)\n+\n     values: List[int] = []\n-    for x in ast.walk(node):\n-        if isinstance(x, (ast.stmt, ast.ExceptHandler)):\n-            values.append(x.lineno - 1)\n-            for name in (\"finalbody\", \"orelse\"):\n-                val: Optional[List[ast.stmt]] = getattr(x, name, None)\n-                if val:\n-                    # Treat the finally/orelse part as its own statement.\n-                    values.append(val[0].lineno - 1 - 1)\n+    \n+    # Use version-specific logic\n+    if sys.version_info >= (3, 9):\n+        add_linenos(values, node, \"finalbody\", \"orelse\")\n+    else:\n+        add_linenos(values, node, \"finalbody\", \"orelse\")\n+\n     values.sort()\n     insert_index = bisect_right(values, lineno)\n     start = values[insert_index - 1]", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-10508", "model_patch": "diff --git a/sklearn/preprocessing/label.py b/sklearn/preprocessing/label.py\n--- a/sklearn/preprocessing/label.py\n+++ b/sklearn/preprocessing/label.py\n@@ -126,6 +126,9 @@ def transform(self, y):\n         \"\"\"\n         check_is_fitted(self, 'classes_')\n         y = column_or_1d(y, warn=True)\n+        # If the input is an empty array, return an empty array.\n+        if _num_samples(y) == 0:\n+            return np.array([])\n \n         classes = np.unique(y)\n         if len(np.intersect1d(classes, self.classes_)) < len(classes):\n@@ -147,6 +150,10 @@ def inverse_transform(self, y):\n         y : numpy array of shape [n_samples]\n         \"\"\"\n         check_is_fitted(self, 'classes_')\n+        y = column_or_1d(y, warn=True)\n+        # If the input is an empty array, return an empty array.\n+        if _num_samples(y) == 0:\n+            return np.array([])\n \n         diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n         if len(diff):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-15512", "model_patch": "diff --git a/sklearn/cluster/_affinity_propagation.py b/sklearn/cluster/_affinity_propagation.py\n--- a/sklearn/cluster/_affinity_propagation.py\n+++ b/sklearn/cluster/_affinity_propagation.py\n@@ -194,17 +194,19 @@ def affinity_propagation(S, preference=None, convergence_iter=15, max_iter=200,\n             unconverged = (np.sum((se == convergence_iter) + (se == 0))\n                            != n_samples)\n             if (not unconverged and (K > 0)) or (it == max_iter):\n+                never_converged = False\n                 if verbose:\n                     print(\"Converged after %d iterations.\" % it)\n                 break\n     else:\n+        never_converged = True\n         if verbose:\n             print(\"Did not converge\")\n \n     I = np.flatnonzero(E)\n     K = I.size  # Identify exemplars\n \n-    if K > 0:\n+    if K > 0 and not never_converged:\n         c = np.argmax(S[:, I], axis=1)\n         c[I] = np.arange(K)  # Identify clusters\n         # Refine the final set of exemplars and clusters and return results\n@@ -408,6 +410,7 @@ def predict(self, X):\n             Cluster labels.\n         \"\"\"\n         check_is_fitted(self)\n+        X = check_array(X)  # Ensure X is a valid array\n         if not hasattr(self, \"cluster_centers_\"):\n             raise ValueError(\"Predict method is not supported when \"\n                              \"affinity='precomputed'.\")", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-25638", "model_patch": "diff --git a/sklearn/utils/multiclass.py b/sklearn/utils/multiclass.py\n--- a/sklearn/utils/multiclass.py\n+++ b/sklearn/utils/multiclass.py\n@@ -155,14 +155,25 @@ def is_multilabel(y):\n     if hasattr(y, \"__array__\") or isinstance(y, Sequence) or is_array_api:\n         # DeprecationWarning will be replaced by ValueError, see NEP 34\n         # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n+        check_y_kwargs = dict(\n+            accept_sparse=True,\n+            allow_nd=True,\n+            force_all_finite=False,\n+            ensure_2d=False,\n+            ensure_min_samples=0,\n+            ensure_min_features=0,\n+        )\n         with warnings.catch_warnings():\n             warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n             try:\n-                y = xp.asarray(y)\n-            except (np.VisibleDeprecationWarning, ValueError):\n+                y = check_array(y, dtype=None, **check_y_kwargs)\n+            except (np.VisibleDeprecationWarning, ValueError) as e:\n+                if str(e).startswith(\"Complex data not supported\"):\n+                    raise\n+\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n-                y = xp.asarray(y, dtype=object)\n+                y = check_array(y, dtype=object, **check_y_kwargs)\n \n     if not (hasattr(y, \"shape\") and y.ndim == 2 and y.shape[1] > 1):\n         return False\n@@ -302,15 +313,27 @@ def type_of_target(y, input_name=\"\"):\n     # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n     # We therefore catch both deprecation (NumPy < 1.24) warning and\n     # value error (NumPy >= 1.24).\n+    check_y_kwargs = dict(\n+        accept_sparse=True,\n+        allow_nd=True,\n+        force_all_finite=False,\n+        ensure_2d=False,\n+        ensure_min_samples=0,\n+        ensure_min_features=0,\n+    )\n+\n     with warnings.catch_warnings():\n         warnings.simplefilter(\"error\", np.VisibleDeprecationWarning)\n         if not issparse(y):\n             try:\n-                y = xp.asarray(y)\n-            except (np.VisibleDeprecationWarning, ValueError):\n+                y = check_array(y, dtype=None, **check_y_kwargs)\n+            except (np.VisibleDeprecationWarning, ValueError) as e:\n+                if str(e).startswith(\"Complex data not supported\"):\n+                    raise\n+\n                 # dtype=object should be provided explicitly for ragged arrays,\n                 # see NEP 34\n-                y = xp.asarray(y, dtype=object)\n+                y = check_array(y, dtype=object, **check_y_kwargs) \n \n     # The old sequence of sequences format\n     try:", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "scikit-learn__scikit-learn-25747", "model_patch": "diff --git a/sklearn/utils/_set_output.py b/sklearn/utils/_set_output.py\n--- a/sklearn/utils/_set_output.py\n+++ b/sklearn/utils/_set_output.py\n@@ -96,6 +96,8 @@ def _get_output_config(method, estimator=None):\n     return {\"dense\": dense_config}\n \n \n+import warnings\n+\n def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):\n     \"\"\"Wrap output with container based on an estimator's or global config.\n \n@@ -127,9 +129,18 @@ def _wrap_data_with_container(method, data_to_wrap, original_input, estimator):\n         return data_to_wrap\n \n     # dense_config == \"pandas\"\n+    index = getattr(original_input, \"index\", None)\n+    if index is not None and len(data_to_wrap) != len(index):\n+        warnings.warn(\n+            \"Length mismatch: Expected axis length does not match the number of rows in transformed data. \"\n+            \"Index will not be set.\",\n+            UserWarning,\n+        )\n+        index = None\n+\n     return _wrap_in_pandas_container(\n         data_to_wrap=data_to_wrap,\n-        index=getattr(original_input, \"index\", None),\n+        index=index,\n         columns=estimator.get_feature_names_out,\n     )", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sphinx-doc__sphinx-11445", "model_patch": "diff --git a/sphinx/util/rst.py b/sphinx/util/rst.py\n--- a/sphinx/util/rst.py\n+++ b/sphinx/util/rst.py\n@@ -82,6 +82,9 @@ def prepend_prolog(content: StringList, prolog: str) -> None:\n         for line in content:\n             if docinfo_re.match(line):\n                 pos += 1\n+            elif line.startswith(':') and '::' not in line:\n+                # Handle domain directives as first heading\n+                break\n             else:\n                 break\n \n@@ -96,6 +99,12 @@ def prepend_prolog(content: StringList, prolog: str) -> None:\n \n         content.insert(pos + lineno + 1, '', '<generated>', 0)\n \n+        # Ensure the first heading is treated correctly when it's a domain directive\n+        if content[pos].startswith(':') and '::' not in content[pos]:\n+            if ' ' in content[pos]:\n+                domain_directive_heading = content[pos].split(' ', 1)[1]\n+                content[pos] = f'{domain_directive_heading}\\n{\"=\" * len(domain_directive_heading)}'\n+\n \n def append_epilog(content: StringList, epilog: str) -> None:\n     \"\"\"Append a string to content body as epilog.\"\"\"", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sphinx-doc__sphinx-8273", "model_patch": "diff --git a/sphinx/builders/manpage.py b/sphinx/builders/manpage.py\n--- a/sphinx/builders/manpage.py\n+++ b/sphinx/builders/manpage.py\n@@ -80,10 +80,14 @@ class ManualPageBuilder(Builder):\n             docsettings.authors = authors\n             docsettings.section = section\n \n-            targetname = '%s.%s' % (name, section)\n+            import os  # Import os module\n+            section_dir = path.join(self.outdir, f'man{section}')\n+            targetname = path.join(section_dir, f'{name}.{section}')\n             logger.info(darkgreen(targetname) + ' { ', nonl=True)\n+            if not path.exists(section_dir):\n+                os.makedirs(section_dir)\n             destination = FileOutput(\n-                destination_path=path.join(self.outdir, targetname),\n+                destination_path=targetname,\n                 encoding='utf-8')\n \n             tree = self.env.get_doctree(docname)", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-14317", "model_patch": "diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -14,6 +14,7 @@\n from sympy.core.containers import Tuple\n from sympy.logic.boolalg import true\n from sympy.core.function import UndefinedFunction, AppliedUndef\n+from collections.abc import Mapping, Iterable, MutableSet, Callable\n \n ## sympy.printing imports\n from sympy.printing.precedence import precedence_traditional\n@@ -1853,8 +1854,16 @@ def _print_PolynomialRingBase(self, expr):\n         return r\"%s%s\\left[%s\\right]\" % (inv, domain, symbols)\n \n     def _print_Poly(self, poly):\n+        from sympy import Mul, S  # Add required imports\n         cls = poly.__class__.__name__\n-        expr = self._print(poly.as_expr())\n+        \n+        # Sort the monomials by their degree in descending order\n+        monomials = poly.terms()\n+        sorted_monomials = sorted(monomials, key=lambda term: sum(term[0]), reverse=True)\n+        sorted_poly_expr = sum([(coeff * Mul(*[gen**deg for gen, deg in zip(poly.gens, degrees)])) \n+                               for degrees, coeff in sorted_monomials], S.Zero)\n+\n+        expr = self._print(sorted_poly_expr)\n         gens = list(map(self._print, poly.gens))\n         domain = \"domain=%s\" % self._print(poly.get_domain())", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-14396", "model_patch": "diff --git a/sympy/polys/polyoptions.py b/sympy/polys/polyoptions.py\n--- a/sympy/polys/polyoptions.py\n+++ b/sympy/polys/polyoptions.py\n@@ -408,6 +408,7 @@ class Domain(with_metaclass(OptionType, Option)):\n     _re_polynomial = re.compile(r\"^(Z|ZZ|Q|QQ)\\[(.+)\\]$\")\n     _re_fraction = re.compile(r\"^(Z|ZZ|Q|QQ)\\((.+)\\)$\")\n     _re_algebraic = re.compile(r\"^(Q|QQ)\\<(.+)\\>$\")\n+    _re_realpoly = re.compile(r\"^(RR)\\[(.+)\\]$\")  # new regex pattern for RR[gens]\n \n     @classmethod\n     def preprocess(cls, domain):\n@@ -480,6 +481,12 @@ def preprocess(cls, domain):\n                 gens = list(map(sympify, r.groups()[1].split(',')))\n                 return sympy.polys.domains.QQ.algebraic_field(*gens)\n \n+            r = cls._re_realpoly.match(domain)  # new matching logic for RR[gens]\n+\n+            if r is not None:\n+                gens = list(map(sympify, r.groups()[1].split(',')))\n+                return sympy.polys.domains.RR.poly_ring(*gens)\n+\n         raise OptionError('expected a valid domain specification, got %s' % domain)\n \n     @classmethod", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-16792", "model_patch": "diff --git a/sympy/utilities/codegen.py b/sympy/utilities/codegen.py\n--- a/sympy/utilities/codegen.py\n+++ b/sympy/utilities/codegen.py\n@@ -695,6 +695,11 @@ def routine(self, name, expr, argument_sequence=None, global_vars=None):\n         arg_list = []\n \n         # setup input argument list\n+\n+        # A helper function to retrieve dimensions for array-like arguments\n+        def dimensions(s):\n+            return [(S.Zero, dim - 1) for dim in s.shape]\n+\n         array_symbols = {}\n         for array in expressions.atoms(Indexed) | local_expressions.atoms(Indexed):\n             array_symbols[array.base.label] = array\n@@ -703,11 +708,8 @@ def routine(self, name, expr, argument_sequence=None, global_vars=None):\n \n         for symbol in sorted(symbols, key=str):\n             if symbol in array_symbols:\n-                dims = []\n                 array = array_symbols[symbol]\n-                for dim in array.shape:\n-                    dims.append((S.Zero, dim - 1))\n-                metadata = {'dimensions': dims}\n+                metadata = {'dimensions': dimensions(array)}\n             else:\n                 metadata = {}\n \n@@ -739,7 +741,11 @@ def routine(self, name, expr, argument_sequence=None, global_vars=None):\n                 try:\n                     new_args.append(name_arg_dict[symbol])\n                 except KeyError:\n-                    new_args.append(InputArgument(symbol))\n+                    if isinstance(symbol, (IndexedBase, MatrixSymbol)):\n+                        metadata = {'dimensions': dimensions(symbol)}\n+                    else:\n+                        metadata = {}\n+                    new_args.append(InputArgument(symbol, **metadata))\n             arg_list = new_args\n \n         return Routine(name, arg_list, return_val, local_vars, global_vars)", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-16988", "model_patch": "diff --git a/sympy/sets/sets.py b/sympy/sets/sets.py\n--- a/sympy/sets/sets.py\n+++ b/sympy/sets/sets.py\n@@ -1260,7 +1260,7 @@ def __new__(cls, *args, **kwargs):\n         evaluate = kwargs.get('evaluate', global_evaluate[0])\n \n         # flatten inputs to merge intersections and iterables\n-        args = _sympify(args)\n+        args = list(ordered(set(_sympify(args))))  # ensuring uniqueness in inputs\n \n         # Reduce sets using known rules\n         if evaluate:", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-20590", "model_patch": "diff --git a/sympy/core/symbol.py b/sympy/core/symbol.py\n--- a/sympy/core/symbol.py\n+++ b/sympy/core/symbol.py\n@@ -197,6 +197,8 @@ class Symbol(AtomicExpr, Boolean):\n     True\n \n     \"\"\"\n+    \n+    __slots__ = ('name',)\n \n     is_comparable = False\n \n@@ -366,6 +368,8 @@ class Dummy(Symbol):\n     _Dummy_10\n \n     \"\"\"\n+    \n+    __slots__ = ('dummy_index',)\n \n     # In the rare event that a Dummy object needs to be recreated, both the\n     # `name` and `dummy_index` should be passed.  This is used by `srepr` for\n@@ -436,6 +440,8 @@ class Wild(Symbol):\n         need to return ``True`` in order for the Wild instance\n         to match the expression.\n \n+    __slots__ = ('exclude', 'properties')\n+\n     Examples\n     ========", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-21171", "model_patch": "diff --git a/sympy/printing/latex.py b/sympy/printing/latex.py\n--- a/sympy/printing/latex.py\n+++ b/sympy/printing/latex.py\n@@ -1968,10 +1968,12 @@ def _print_DiracDelta(self, expr, exp=None):\n             tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n         return tex\n \n-    def _print_SingularityFunction(self, expr):\n+    def _print_SingularityFunction(self, expr, exp=None):\n         shift = self._print(expr.args[0] - expr.args[1])\n         power = self._print(expr.args[2])\n         tex = r\"{\\left\\langle %s \\right\\rangle}^{%s}\" % (shift, power)\n+        if exp is not None:\n+            tex = r\"\\left(%s\\right)^{%s}\" % (tex, exp)\n         return tex\n \n     def _print_Heaviside(self, expr, exp=None):", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-22840", "model_patch": "diff --git a/sympy/simplify/cse_main.py b/sympy/simplify/cse_main.py\n--- a/sympy/simplify/cse_main.py\n+++ b/sympy/simplify/cse_main.py\n@@ -567,6 +567,7 @@ def tree_cse(exprs, symbols, opt_subs=None, order='canonical', ignore=():\n         Substitutions containing any Symbol from ``ignore`` will be ignored.\n     \"\"\"\n     from sympy.matrices.expressions import MatrixExpr, MatrixSymbol, MatMul, MatAdd\n+    from sympy.matrices.expressions.matexpr import MatrixElement\n     from sympy.polys.rootoftools import RootOf\n \n     if opt_subs is None:\n@@ -586,7 +587,10 @@ def _find_repeated(expr):\n         if isinstance(expr, RootOf):\n             return\n \n-        if isinstance(expr, Basic) and (expr.is_Atom or expr.is_Order):\n+        if isinstance(expr, Basic) and (\n+                expr.is_Atom or\n+                expr.is_Order or\n+                isinstance(expr, (MatrixSymbol, MatrixElement))):\n             if expr.is_Symbol:\n                 excluded_symbols.add(expr)\n             return", "model_name_or_path": "AppMap Navie GPT 4o"}
{"instance_id": "sympy__sympy-24102", "model_patch": "diff --git a/sympy/parsing/mathematica.py b/sympy/parsing/mathematica.py\n--- a/sympy/parsing/mathematica.py\n+++ b/sympy/parsing/mathematica.py\n@@ -654,7 +654,7 @@ def _from_mathematica_to_tokens(self, code: str):\n             code_splits[i] = code_split\n \n         # Tokenize the input strings with a regular expression:\n-        token_lists = [tokenizer.findall(i) if isinstance(i, str) else [i] for i in code_splits]\n+        token_lists = [tokenizer.findall(i) if isinstance(i, str) and i.isascii() else [i] for i in code_splits]\n         tokens = [j for i in token_lists for j in i]\n \n         # Remove newlines at the beginning", "model_name_or_path": "AppMap Navie GPT 4o"}
